<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Christian Duffau-Rasmussen">
  <title>Gradient Boosting: How does it work?</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #cccccc; background-color: #303030; }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span.al { color: #ffcfaf; } /* Alert */
    code span.an { color: #7f9f7f; font-weight: bold; } /* Annotation */
    code span.at { } /* Attribute */
    code span.bn { color: #dca3a3; } /* BaseN */
    code span.bu { } /* BuiltIn */
    code span.cf { color: #f0dfaf; } /* ControlFlow */
    code span.ch { color: #dca3a3; } /* Char */
    code span.cn { color: #dca3a3; font-weight: bold; } /* Constant */
    code span.co { color: #7f9f7f; } /* Comment */
    code span.cv { color: #7f9f7f; font-weight: bold; } /* CommentVar */
    code span.do { color: #7f9f7f; } /* Documentation */
    code span.dt { color: #dfdfbf; } /* DataType */
    code span.dv { color: #dcdccc; } /* DecVal */
    code span.er { color: #c3bf9f; } /* Error */
    code span.ex { } /* Extension */
    code span.fl { color: #c0bed1; } /* Float */
    code span.fu { color: #efef8f; } /* Function */
    code span.im { } /* Import */
    code span.in { color: #7f9f7f; font-weight: bold; } /* Information */
    code span.kw { color: #f0dfaf; } /* Keyword */
    code span.op { color: #f0efd0; } /* Operator */
    code span.ot { color: #efef8f; } /* Other */
    code span.pp { color: #ffcfaf; font-weight: bold; } /* Preprocessor */
    code span.sc { color: #dca3a3; } /* SpecialChar */
    code span.ss { color: #cc9393; } /* SpecialString */
    code span.st { color: #cc9393; } /* String */
    code span.va { } /* Variable */
    code span.vs { color: #cc9393; } /* VerbatimString */
    code span.wa { color: #7f9f7f; font-weight: bold; } /* Warning */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/theme/black.css" id="theme">
  <link rel="stylesheet" href="../black-root.css"/>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Gradient Boosting:<br>How does it work?</h1>
  <p class="author">Christian Duffau-Rasmussen</p>
  <p class="date">9. November 2021</p>
</section>

<section id="section" class="slide level3">
<h3></h3>
<div class="columns">
<div class="column" style="width:50%;">
<p><a href="https://ante.dk"><img data-src="../static/ante_logo_neg.svg" style="height:20.0%" /></a></p>
</div><div class="column" style="width:50%;">
<ul>
<li class="fragment"><p>Data Scientist and Co-founder</p></li>
<li class="fragment"><p>Legal tech start-up</p></li>
<li class="fragment"><p>NLP-powered search engine</p></li>
<li class="fragment"><p>Extract meta data from plain text</p></li>
<li class="fragment"><p>Index structured data in a search engine</p></li>
</ul>
</div>
</div>
</section>
<section id="boosting---what-is-it" class="title-slide slide level2">
<h2>Boosting - What is it ?</h2>
<blockquote>
<p>A procedure to combine many <strong>weak</strong> learners to produce a powerful <strong>committee</strong>.</p>
<p>Elements of Statistical Learning <span class="citation" data-cites="friedman2009elements">(J. Friedman et al. 2009, sec. 10.1)</span></p>
</blockquote>
<aside class="notes">
<ul>
<li>An ensemble method in Machine Learning</li>
<li>A way of combining so-called weak learner</li>
<li>into a powerful comittee i.e. by some means og aggregation or averaging</li>
</ul>
</aside>
</section>

<section>
<section id="a-brief-history" class="title-slide slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/leslie-valiant.jpg" height="130" /></p>
<p><span class="citation" data-cites="valiant1984theory">(Valiant 1984)</span></p>
<ul>
<li class="fragment">Probably Approximately Correct Learner (PAC)<br />
</li>
<li class="fragment">A formal theory of learnability</li>
<li class="fragment">Proves broad classes of non-trivial boolean functions are learnable</li>
<li class="fragment">Turing Award in 2010</li>
</ul>
</div>
</div>
<aside class="notes">
<ul>
<li>Leslie Valiant - Harvard</li>
<li>Leslie Valiant (1984) - A theory of the learnable</li>
<li>Defines a mathematical framework for analyzing what classes of problems are learnable in polynomial time.</li>
<li>Introduces the Probably Approximately Correct Learner (PAC-learner).</li>
<li>Foundation of the field of computational learning theory.</li>
<li>The PAC leaner is to
<ul>
<li>computational learning theory what</li>
</ul></li>
<li>The Turing machine is to
<ul>
<li>computational complexity theory</li>
</ul></li>
</ul>
</aside>
</section>
<section id="side-note-pac-learning" class="slide level3">
<h3>Side note: PAC learning</h3>
<ul>
<li class="fragment"><p>Boolean <em>concept</em> : <span class="math inline">\((0,1,1,0,\cdots, 1) \mapsto 0 \text{ or } 1\)</span></p></li>
<li class="fragment"><p>A <em>concept</em> is Probably Approximately Learnable</p>
<ul>
<li class="fragment"><em>if</em> an <em>algorithm</em> can deduce a <em>hypothesis</em> (aka. a function)</li>
<li class="fragment">in time bounded by a polynomial of the size of <em>concept</em> in bits</li>
<li class="fragment">with <span class="math display">\[P\left(\text{error-rate} &lt;\varepsilon\right) &gt; \delta\]</span></li>
<li class="fragment"><em>for all</em> <span class="math inline">\(\varepsilon&gt;0\)</span> and <span class="math inline">\(0 &lt; \delta \leq 1\)</span></li>
</ul></li>
<li class="fragment"><p>The <em>learning protocol</em> is:</p>
<ul>
<li class="fragment">learn from examples asking an ORACLE</li>
<li class="fragment">the ORACLE returns a random example in <em>unit</em> time (O(1))</li>
</ul></li>
</ul>
</section></section>
<section>
<section id="a-brief-history-1" class="title-slide slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/leslie-valiant.jpg" height="130" alt="Leslie Valiant" /> <img data-src="../static/michael-kearns.jpg" height="130" alt="Michael Kearns" /></p>
<p><span class="citation" data-cites="kearns1989crytographic">(Kearns and Valiant 1989)</span></p>
<ul>
<li class="fragment">Introduce <em>weak learner</em><br />
</li>
<li class="fragment">Performs only slightly better than chance</li>
<li class="fragment"><em>Hypothesis boosting problem</em>: weak &lt;=&gt; strong ?</li>
</ul>
</div>
</div>
<aside class="notes">
<ul>
<li><p>Michael Kearns &amp; Leslie Valiant (ph.d. student and supervisor) - Harvard University</p></li>
<li><p>The 1989 Crypto paper show that</p></li>
<li><p>If either:</p>
<ul>
<li>Boolean formulae</li>
<li>Deterministic finite automata</li>
<li>Constant-depth threshold circuits</li>
</ul></li>
<li><p>are learnable then cryptography is toast.</p></li>
<li><p>Kearns and Valiant state as an open problem:</p>
<ul>
<li>Can weak learners be “boosted” into strong learners?</li>
<li>I.e. can an algorithm transform weak learners in to strong ones</li>
<li>The notion at the time was “probably not”</li>
</ul></li>
</ul>
</aside>
</section>
<section id="side-note-weak-and-strong-learners" class="slide level3">
<h3>Side note: <em>weak</em> and <em>strong</em> learners</h3>
<ul>
<li class="fragment"><p><em>strongly learnable</em> == PAC learnable</p></li>
<li class="fragment"><p>A <em>concept</em> is <em>weakly</em> Learnable</p>
<ul>
<li class="fragment"><em>if</em> an <em>algorithm</em> can deduce a <em>hypothesis</em></li>
<li class="fragment">in time bounded by a polynomial of the size of <em>concept</em> in bits (<span class="math inline">\(s\)</span>)</li>
<li class="fragment">with <span class="math display">\[P\left(\text{error-rate} &lt; \frac{1}{2} - 1/p(s)\right) &gt; \delta\]</span></li>
<li class="fragment"><em>for all</em> <span class="math inline">\(0 &lt; \delta \leq 1\)</span></li>
</ul></li>
</ul>
</section></section>
<section id="a-brief-history-2" class="title-slide slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/robert-schapire.jpg" height="130" /></p>
<p><span class="citation" data-cites="schapire1990strength">(R. E. Schapire 1990)</span></p>
<ul>
<li class="fragment">Cracks the <em>Hypothesis boosting problem</em></li>
<li class="fragment">Shows <em>weak learner</em> &lt;=&gt; <em>strong learner</em></li>
<li class="fragment">An algorithm constructing a strong learner from weak ones 🤯</li>
</ul>
</div>
</div>
<aside class="notes">
<ul>
<li>Robert E Schapire - MIT,later professor at Princeton - 27 years at the time</li>
<li>Ph.d. thesis</li>
</ul>
</aside>
</section>

<section id="a-brief-history-3" class="title-slide slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/yoav-freund.png" height="130" /></p>
<p><span class="citation" data-cites="freund1990majority">(Freund 1990)</span></p>
<ul>
<li class="fragment">Youav Freund - Israeli - p.hd. - UC Santa Cruz - 29 years at the time</li>
<li class="fragment">Ph.d. thesis</li>
<li class="fragment">Implements a much more efficient <em>boosting</em> algorithm</li>
<li class="fragment">Trains learners on weighted subsets of the data</li>
<li class="fragment">Uses majority voting to predict</li>
</ul>
</div>
</div>
</section>

<section id="a-brief-history-4" class="title-slide slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/yoav-freund.png" height="130" /> <img data-src="../static/robert-schapire.jpg" height="130" /></p>
<p><span class="citation" data-cites="schapire1995decision">(R. Schapire and Freund 1995)</span></p>
<ul>
<li class="fragment">Youav Freund - Israeli - Post Doc at UC San Diego - 34 years at the time</li>
<li class="fragment">Robert E Schapire - Post doc at Princeton - 32 years at the time</li>
<li class="fragment">Introduces AdaBoost</li>
<li class="fragment">First practical boosting algorithm<br />
</li>
<li class="fragment">Winners of Gödel prize in 2003</li>
</ul>
</div>
</div>
<aside class="notes">
<ul>
<li>Motivate AdaBoost by an Hedging example</li>
<li>Choose the allocation of money to gamblers placing bets on your behalf</li>
<li>Minimize difference to best performing gambler in a online scheme</li>
<li>Translate the hedging algo to a training setup</li>
<li>Derive bounds on errors of the Adaboost outputted hypothesis</li>
</ul>
</aside>
</section>

<section id="a-brief-history-5" class="title-slide slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/jerome-h-friedman.jpeg" height="130" /> <img data-src="../static/robert-tibshirani-trevor-hastie.jpg" height="130" /></p>
<p><span class="citation" data-cites="friedman2000special">(J. Friedman, Hastie, and Tibshirani 2000)</span></p>
<ul>
<li class="fragment">Shows AdaBoost is <em>Stagewise Additive Logistic regression</em></li>
</ul>
</div>
</div>
<aside class="notes">
<ul>
<li>Jerome Harold Friedman - American - Professor of Statistics at Stanford University - 61 years at the time</li>
<li>Trevor Hastie - South Africa - Professor in Statistics at Stanford University - 47 years at the time</li>
<li>Robert Tibshirani - Canadian - Professor in Statistics at Stanford University - 44 years at the time</li>
</ul>
</aside>
</section>

<section id="a-brief-history-6" class="title-slide slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/jerome-h-friedman.jpeg" height="130" /></p>
<p><span class="citation" data-cites="friedman2001greedy">(J. H. Friedman 2001)</span> <span class="citation" data-cites="mason1999boosting">(Mason et al. 1999)</span></p>
<ul>
<li class="fragment">Generalizes the <em>boosting</em> concept</li>
<li class="fragment">Describes <em>boosting</em> as gradient descent in function space</li>
</ul>
</div>
</div>
</section>

<section id="a-brief-history-7" class="title-slide slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/tianqi-chen.jpg" height="130" /></p>
<p><span class="citation" data-cites="chen2015higgs">(Chen and He 2015)</span></p>
<ul>
<li class="fragment">Wins Kaggle contest on Higgs Boson using XGBoost</li>
<li class="fragment">XGBoost quickly becomes the most winning algorithm</li>
</ul>
</div>
</div>
<aside class="notes">
<ul>
<li>Tianqi Chen - Shanghai China - Ph. d. at Univeristy of Washington</li>
</ul>
</aside>
</section>

<section id="decision-trees" class="title-slide slide level2">
<h2>Decision trees</h2>
<div class="columns">
<div class="column" style="width:66%;">
<figure>
<img data-src="../static/partitions-and-trees.png" style="width:80.0%" alt="Top left: General partition, Top right: recursive binary splits" /><figcaption aria-hidden="true">Top left: General partition, Top right: recursive binary splits</figcaption>
</figure>
</div><div class="column" style="width:33%;">
<ul>
<li class="fragment">Partitions feature space into rectangles</li>
<li class="fragment">Finding Optimal trees is NP-complete</li>
<li class="fragment">Greedy algorithms and heuristics are used when fitting (CART, ID3, C4.5, etc.)</li>
<li class="fragment">Trees are fast when predicting</li>
</ul>
</div>
</div>
<aside class="notes">
<ul>
<li>Scikit-learn uses an optimized version of CART</li>
<li>Heuristics are used to approximate a optimal solution</li>
<li>Most fitting procedures uses some kind of recursive binary splitting</li>
</ul>
</aside>
</section>

<section id="ensemble-methods" class="title-slide slide level2">
<h2>Ensemble methods</h2>
<ul>
<li class="fragment">Bagging: Grow trees using <em>random subsets of data (with replacement)</em></li>
<li class="fragment">Random forest: Grow trees using <em>random subset of features</em></li>
<li class="fragment">Boosting: Grow trees on <em>re-weighted dataset</em></li>
</ul>
<div class="fragment">
<p><span class="math display">\[\text{Boosting} \succ \text{Random forest} \succ \text{Bagging} \succ \text{Tree}\]</span></p>
<aside class="notes">
<ul>
<li>Ensemble methods overcome overfitting by combining trees (typically)</li>
</ul>
</aside>
</div>
</section>

<section id="why-ensembles-of-decision-trees" class="title-slide slide level2">
<h2>Why Ensembles of Decision trees?</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../plots/tree-depth.svg" style="width:90.0%" /></p>
</div><div class="column" style="width:33%;">
<ul>
<li class="fragment">Nonlinearity</li>
<li class="fragment">Feature interaction:
<ul>
<li class="fragment">1 layer: <span class="math inline">\(f(X_i)\)</span></li>
<li class="fragment">2 layers: <span class="math inline">\(f(X_i, X_j)\)</span></li>
<li class="fragment">3 layers: <span class="math inline">\(f(X_i, X_j, X_k)\)</span></li>
</ul></li>
<li class="fragment">Automatic feature selection</li>
<li class="fragment">Single Trees overfit</li>
<li class="fragment">Combining many small trees …</li>
<li class="fragment">.. you get <em>flexible</em> yet <em>robust</em> models</li>
</ul>
</div>
</div>
<aside class="notes">
<ul>
<li>Ensemble methods most often use trees</li>
<li>But why?</li>
<li>Shallow Trees are weak learners, so don’t overfit</li>
<li>Shallow Trees still model interactions</li>
<li>Linear model cannot model interactions</li>
<li>Automatic feature selection</li>
</ul>
</aside>
</section>

<section id="simulation-example" class="title-slide slide level2">
<h2>Simulation example</h2>
<p><span class="math display">\[Y = \begin{cases}
1 &amp; \text{if}\quad X_1^2 + \ldots + X_{10}^2 &gt; 9.34 \\
-1 &amp; \text{else}
\end{cases}\quad X_i\sim N(0,1)\]</span></p>
<figure>
<img data-src="simulation/plots/gen_data.png" height="400" alt="Simulated 10-D nested spheres" /><figcaption aria-hidden="true">Simulated 10-D nested spheres</figcaption>
</figure>
</section>

<section id="simulation-example-1" class="title-slide slide level2">
<h2>Simulation example</h2>
<p><img data-src="simulation/plots/ensemble_test_errors.svg" /></p>
<aside class="notes">
<ul>
<li>Adaboost outperforms the other ensemble methods with stumps vs full trees</li>
</ul>
</aside>
</section>

<section id="simulation-example-2" class="title-slide slide level2">
<h2>Simulation example</h2>
<p><img data-src="simulation/plots/adaboost_errors.svg" /></p>
<aside class="notes">
<ul>
<li>Adaboost test error decreases after training error has hit <em>zero</em></li>
<li>Adaboost learns something more general than just the training data “by heart”</li>
</ul>
</aside>
</section>

<section id="biasvariance-trade-off" class="title-slide slide level2">
<h2>Bias/variance trade-off</h2>
<p><span class="math display">\[\text{MSE} = \text{Bias}(\hat{f})^2 + \text{Var}(\hat{f}) + \text{Irreducible noise}\]</span></p>
</section>

<section id="variance-of-averages" class="title-slide slide level2">
<h2>Variance of averages</h2>
<p><span class="math display">\[\text{Var}\left(\frac{1}{n}\sum_i^n X_i\right) = \frac{1}{n}\sum_i^n \text{Var}\left(X_i\right) + \frac{1}{n}\sum_{i\neq j} \text{Cov}(X_i, X_j)\]</span></p>
<div class="fragment">
<p><span class="math display">\[\text{Variance of ensemble} = \frac{\text{Var(Trees)}}{n} + \frac{\text{Cov( Trees)}}{n}\]</span></p>
</div>
</section>

<section id="variance-and-bias-reduction" class="title-slide slide level2">
<h2>Variance and bias reduction</h2>
<figure>
<img data-src="simulation/plots/consecutive_predictions_corr.svg" alt="Classifying 10-D nested spheres" /><figcaption aria-hidden="true">Classifying 10-D nested spheres</figcaption>
</figure>
<aside class="notes">
<ul>
<li>Bagging: Training data subsets correlates. Mellow positive correlation between all models.</li>
<li>Random forest:
<ul>
<li>Either correlation of 1: Set of features with equal importance</li>
<li>Correlation of zero: Features which are uncorrelated</li>
</ul></li>
<li>Adaboost: Zero or negative correlation. Out of thin air.</li>
</ul>
</aside>
</section>

<section id="boosting" class="title-slide slide level2">
<h2>Boosting</h2>
<p><img data-src="../static/Ensemble_Boosting.svg" /></p>
<aside class="notes">
<ul>
<li>A collection of <em>weak learners</em> (e.g. classifier) are trained sequentially.</li>
<li>Each <em>learner</em> is trained on the same dataset.</li>
<li>Each example is re-weighted in each iteration</li>
<li>Poorly predicted examples get higher weight</li>
<li>Well predicted examples get lower weight</li>
</ul>
</aside>
</section>

<section id="adaboost-algorithm" class="title-slide slide level2">
<h2>Adaboost algorithm</h2>
<div class="sourceCode" id="cb1"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> fit(X, y, weak_leaner, iters<span class="op">=</span><span class="dv">100</span>):</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a>  weights <span class="op">=</span> np.ones(<span class="bu">len</span>(y)) <span class="op">*</span> <span class="dv">1</span><span class="op">/</span><span class="bu">len</span>(y)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a>  <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(iters):</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a>      weak_leaner[i].fit(X, y, sample_weight<span class="op">=</span>w)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>      y_pred <span class="op">=</span> weak_leaner[i].predict(X)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>      errors <span class="op">=</span> (y_pred <span class="op">!=</span> y)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a>      error_rate <span class="op">=</span> <span class="bu">sum</span>(weights <span class="op">*</span> errors)<span class="op">/</span><span class="bu">sum</span>(w)</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>      alphas[i] <span class="op">=</span> log((<span class="dv">1</span> <span class="op">-</span> error_rate)<span class="op">/</span>error_rate)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>      weights <span class="op">=</span> weights <span class="op">*</span> errors <span class="op">*</span> exp(alpha)</span></code></pre></div>
<ol type="1">
<li class="fragment">Initialize weights <span class="math inline">\(w_i=1/N\)</span></li>
<li class="fragment">For <span class="math inline">\(m=1\)</span> to <span class="math inline">\(M\)</span>:
<ol type="1">
<li class="fragment">Fit classifier <span class="math inline">\(G_m(x)\)</span> to training data using <span class="math inline">\(w_i\)</span>’s</li>
<li class="fragment">Compute <span class="math display">\[\text{err} = \frac{\sum_{i=1}^N w_i I(y_i \neq G_m(x_i))}{\sum_{i=1}^N w_i}\]</span></li>
<li class="fragment">Compute <span class="math inline">\(\alpha_m = \log((1-\text{err})/\text{err})\)</span></li>
<li class="fragment">Set <span class="math inline">\(w_i \leftarrow w_i \exp(\alpha_m I(y_i \neq G_m(x_i)))\)</span></li>
</ol></li>
<li class="fragment">Output <span class="math display">\[G(x) = \text{sign}\left(\sum_{m=1}^M \alpha_m G_m(x)\right)\]</span></li>
</ol>
</section>

<section>
<section id="adaboost" class="title-slide slide level2">
<h2>Adaboost</h2>
<ul>
<li class="fragment">Not originally motivated in <em>Forward Stagewise Additive Learning</em></li>
<li class="fragment"><span class="citation" data-cites="friedman2000special">(J. Friedman, Hastie, and Tibshirani 2000)</span> show that Adaboost is a stagewise additive model with loss <span class="math display">\[L(y, f (x)) = \exp(−y f(x))\]</span> where <span class="math inline">\(y\in \{-1, 1\}\)</span> and <span class="math inline">\(f(x) \in \mathbb{R}\)</span>.</li>
<li class="fragment">Usually for classification we use cross-entropy <span class="math display">\[\begin{align}
L(y, f(x)) &amp;= y^\prime \log p(x) + (1-y^\prime) \log(1- p(x)) \\ 
&amp;= \log(1 + e^{-2yf(x)})
\end{align}\]</span> where <span class="math inline">\(y^\prime = (y+1)/2\in \{0,1\}\)</span> and <span class="math inline">\(p(x)\)</span> is the softmax function.</li>
</ul>
</section>
<section id="cross-entropy-derivation" class="slide level3">
<h3>Cross-entropy derivation</h3>
<p><span class="math display">\[\begin{align}
P(y=1|x) &amp;= p(x) = \frac{e^{f(x)}}{e^{-f(x)}+e^{f(x)}} = \frac{1}{1+e^{-2f(x)}} \\
P(y=-1|x) &amp;= 1-p(x) = \frac{e^{-f(x)}}{e^{-f(x)}+e^{f(x)}} = \frac{1}{1+e^{2f(x)}}
\end{align}\]</span></p>
<p><span class="math display">\[\begin{align}
L(y, f(x)) &amp;= \log p(x) =  \log \frac{1}{1+e^{-2f(x)}} \quad\text{if}\,y=1 \\
L(y, f(x)) &amp;= \log(1- p(x)) = \log \frac{1}{1+e^{2f(x)}} \quad\text{if}\,y=-1  \\ 
\end{align}\]</span> hence the <em>negative log-likelihood</em> is the <em>cross-entropy</em> <span class="math display">\[\begin{align}
L(y, f(x)) &amp;= \log \frac{1}{1+e^{-2yf(x)}}  \Leftrightarrow \\
-L(y, f(x)) &amp;= \log \left(1+e^{-2yf(x)}\right)
\end{align}\]</span></p>
</section></section>
<section id="adaboost-1" class="title-slide slide level2">
<h2>Adaboost</h2>
<ul>
<li class="fragment">In theory the two loss functions are equivalent</li>
<li class="fragment">On average they produce the same <em>model</em> <span class="math inline">\(f\)</span></li>
<li class="fragment">For finite samples the exponential loss has drawbacks</li>
<li class="fragment">To much weight is given to errors</li>
</ul>
<div class="fragment">
<figure>
<img data-src="../plots/loss_functions.svg" style="width:50.0%" alt="Exponential loss and cross-entropy" /><figcaption aria-hidden="true">Exponential loss and cross-entropy</figcaption>
</figure>
</div>
</section>

<section id="adaboost-2" class="title-slide slide level2">
<h2>Adaboost</h2>
<ul>
<li class="fragment">10D nested spheres with noisy data</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<figure>
<img data-src="simulation/plots/gen_data.png" alt="Clean data" /><figcaption aria-hidden="true">Clean data</figcaption>
</figure>
</div><div class="column" style="width:50%;">
<figure>
<img data-src="simulation/plots/gen_noisy_data.png" alt="Noisy data" /><figcaption aria-hidden="true">Noisy data</figcaption>
</figure>
</div>
</div>
</section>

<section id="adaboost-3" class="title-slide slide level2">
<h2>Adaboost</h2>
<ul>
<li class="fragment">Adaboost has trouble on noisy data</li>
</ul>
<div class="fragment">
<div class="columns">
<div class="column" style="width:50%;">
<figure>
<img data-src="simulation/plots/ensemble_test_errors.svg" alt="Test error clean data" /><figcaption aria-hidden="true">Test error clean data</figcaption>
</figure>
</div><div class="column" style="width:50%;">
<figure>
<img data-src="simulation/plots/ensemble_test_errors_noisy.svg" alt="Test error noisy data" /><figcaption aria-hidden="true">Test error noisy data</figcaption>
</figure>
</div>
</div>
<aside class="notes">
<ul>
<li>The deviance yf is only negative if they have different signs</li>
<li>So all negative values are errors</li>
<li>The exponential loss eight these error much heavier</li>
</ul>
</aside>
</div>
</section>

<section id="can-we-fix-adaboost" class="title-slide slide level2">
<h2>Can we fix Adaboost ?</h2>
<ul>
<li class="fragment">Can we easily plug in a more robust loss function?</li>
</ul>
<div class="fragment">
<blockquote>
<div>
<ol type="1">
<li>Initialize weights <span class="math inline">\(w_i=1/N\)</span></li>
<li>For <span class="math inline">\(m=1\)</span> to <span class="math inline">\(M\)</span>:
<ol type="1">
<li>Fit classifier <span class="math inline">\(G_m(x)\)</span> to training data using <span class="math inline">\(w_i\)</span>’s</li>
<li>Compute <span class="math inline">\(\text{err} = \frac{\sum_{i=1}^N w_i I(y_i \neq G_m(x_i))}{\sum_{i=1}^N w_i}\)</span></li>
<li>Compute <span class="math inline">\(\alpha_m = \log((1-\text{err})/\text{err})\)</span></li>
<li>Set <span class="math inline">\(w_i \leftarrow w_i \exp(\alpha_m I(y_i \neq G_m(x_i)))\)</span></li>
</ol></li>
<li>Output <span class="math display">\[G(x) = \text{sign}\left(\sum_{m=1}^M \alpha_m G_m(x)\right)\]</span></li>
</ol>
</div>
</blockquote>
</div>
<div class="fragment">
<p>🤷‍♂️</p>
</div>
</section>

<section>
<section id="forward-stagewise-additive-learning" class="title-slide slide level2">
<h2>Forward Stagewise Additive Learning</h2>
<div class="sourceCode" id="cb2"><pre class="sourceCode python"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> estimator(X):</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>  <span class="cf">pass</span></span></code></pre></div>
</section>
<section id="forward-stagewise-additive-learning-1" class="slide level3">
<h3>Forward Stagewise Additive Learning</h3>
<ol type="1">
<li class="fragment">Initialize <span class="math inline">\(f_0(x) = 0\)</span></li>
<li class="fragment">For <span class="math inline">\(m=1\)</span> to <span class="math inline">\(M\)</span>:
<ol type="a">
<li class="fragment">Compute <span class="math display">\[(\beta_m, \gamma_m) = \underset{\beta, \gamma}{\text{argmin}} \sum_{i=1}^N L(y_i, f_{m-1}(x_i)+ \beta b(x_i;\gamma))\]</span></li>
<li class="fragment">Set <span class="math inline">\(f_m(x) = f_{m-1}(x) + \beta_m b(x; \gamma_m)\)</span></li>
</ol></li>
</ol>
<aside class="notes">
<ul>
<li>Boosting is a special case of <em>Forward Stagewise Additive Learning</em></li>
<li>It’s an approximation technique for general additive models</li>
</ul>
</aside>
</section></section>
<section id="gradient-boosting" class="title-slide slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">Introduced by <span class="citation" data-cites="friedman2001greedy">(J. H. Friedman 2001)</span> as a new view of boosting</li>
<li class="fragment">Makes it possible to <em>derive</em> a boosting procedure</li>
<li class="fragment">Solves each forward stagewise step as <em>gradient descent</em> in a function space</li>
<li class="fragment">Applies to any differentiable loss function</li>
</ul>
</section>

<section id="side-note-gradient-boosting-in-scikit-learn" class="title-slide slide level2">
<h2>Side note: Gradient boosting in SciKit Learn</h2>
<p><em>Scikit-learn documentation</em>:</p>
<blockquote>
<p>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions.</p>
</blockquote>
<div class="fragment">
<p>Sounds promising…</p>
</div>
<div class="fragment">
<blockquote>
<p><code>loss: {‘deviance’, ‘exponential’}</code></p>
<p>For loss ‘exponential’ gradient boosting recovers the AdaBoost algorithm.</p>
</blockquote>
</div>
<div class="fragment">
<p>🤦‍♂️</p>
</div>
</section>

<section id="gradient-boosting-1" class="title-slide slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">Consider the loss function, <span class="math display">\[\sum_{i=1}^N L\left(y_i, \sum_{m=1}^M \beta_m b(x_i, \gamma_m)\right)\]</span></li>
<li class="fragment">… to be optimized over <span class="math inline">\(\{\beta_m, \gamma_m\}_{m=1}^M\)</span></li>
<li class="fragment">… which might me intractable</li>
</ul>
</section>

<section id="gradient-boosting-2" class="title-slide slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">We try to solve it in a Forward stagewise mamner</li>
<li class="fragment">For <span class="math inline">\(1\)</span> to <span class="math inline">\(M\)</span>:
<ul>
<li class="fragment">Optimize <span class="math display">\[\sum_{i=1}^N L\left(y_i, f_{m-1}(x_i) + \beta_m b(x_i, \gamma_m)\right)\]</span></li>
</ul></li>
<li class="fragment">where <span class="math inline">\(f_m(x) = f_{m-1}(x) + \beta_m b(x_i, \gamma_m)\)</span>.</li>
<li class="fragment">If the basis functions are trees</li>
<li class="fragment">… the stagewise minimization ca be difficult</li>
</ul>
</section>

<section id="gradient-boosting-3" class="title-slide slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">Consider the expected loss function <span class="math display">\[\phi(f) = E[L(y,f(\boldsymbol{x}))]\]</span></li>
<li class="fragment">we seek <span class="math inline">\(f\)</span> that minimizes <span class="math inline">\(\phi\)</span></li>
<li class="fragment">Thats a difficult variational analysis problem</li>
<li class="fragment">Let’s circumvent the problem… 😎</li>
<li class="fragment">… consider a dataset of <span class="math inline">\(N\)</span> observations instead <span class="math display">\[\mathbf{f} = \{f(x_1), \ldots, f(x_N)\}\]</span></li>
</ul>
</section>

<section id="gradient-boosting-4" class="title-slide slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">Lets minimize the empirical loss <span class="math display">\[L(f) = \sum_{i=1}^N L(y_i, f(x_i))\]</span> wrt. <span class="math inline">\(f(x_i)\)</span> numerically.</li>
<li class="fragment">All numerical optimization is a series of <span class="math inline">\(M\)</span> steps towards optimum</li>
<li class="fragment">Let each step be a vector <span class="math inline">\(\mathbf{h}_m\in\mathbb{R}^N\)</span></li>
<li class="fragment">The numerical optimum is <span class="math display">\[\mathbf{f}_M = \sum_{m=1}^M \mathbf{h}_m\]</span></li>
</ul>
</section>

<section id="gradient-boosting-5" class="title-slide slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">In <em>gradient descent</em> each step <span class="math inline">\(\mathbf{h}_m\)</span> is <span class="math display">\[-\rho \mathbf{g}_m\]</span></li>
<li class="fragment">… where <span class="math inline">\(g_{im}\)</span> is <span class="math display">\[\left[\frac{\partial L(y_i, f(x_i))}{\partial  f(x_i)}\right]_{f(x)=f_{m-1}(x)}\]</span></li>
<li class="fragment">The numerical optimization is then <span class="math display">\[
\begin{align*}
\mathbf{f}_m &amp;= \mathbf{f}_{m-1} + \mathbf{h}_m\\
&amp;= \mathbf{f}_{m-1} -\rho \mathbf{g}_m 
\end{align*}
\]</span></li>
</ul>
</section>

<section id="gradient-boosting-6" class="title-slide slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">Let’s add <em>line search</em>, <span class="math display">\[\mathbf{f}_m = \mathbf{f}_{m-1} - \rho_m \mathbf{g}_m\]</span> where <span class="math inline">\(\rho_m = \underset{\rho}{\text{argmin}}\quad L(\mathbf{f}_{m-1}-\rho \mathbf{g}_m )\)</span></li>
<li class="fragment">This fits the pattern of <em>forward stagewise learning</em> <span class="math display">\[f_m(x) = f_{m-1}(x) + \beta_m b(x; \gamma_m)\]</span></li>
<li class="fragment">where <span class="math inline">\(\rho_m\)</span> is analogous to <span class="math inline">\(\beta_m\)</span></li>
<li class="fragment">and <span class="math inline">\(-\mathbf{g}_m\)</span> is analogous to <span class="math inline">\(b(x; \gamma_m)\)</span></li>
<li class="fragment">We can derive the gradient considering <span class="math inline">\(f(x)\)</span> a variable</li>
<li class="fragment">… and easily compute <span class="math inline">\(\mathbf{g}_m(f(x))\)</span> using the value from the previous iteration <span class="math inline">\(f_{m-1}(x)\)</span></li>
<li class="fragment">… we cannot compute <span class="math inline">\(\mathbf{g}_m\)</span> outside the dataset 🤔</li>
</ul>
</section>

<section id="gradient-boosting-7" class="title-slide slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">The idea of <span class="citation" data-cites="friedman2001greedy">(J. H. Friedman 2001)</span>:
<ul>
<li class="fragment">Fit a <em>learner</em> to <span class="math inline">\(-\mathbf{g}_m\)</span> <span class="math inline">\(\rightarrow \hat{b}_m(x; \gamma_m)\)</span></li>
<li class="fragment">Forward stagewise step: <span class="math display">\[f_m(x) = f_{m-1}(x) + \rho_m \hat{b}_m(x; \gamma_m)\]</span></li>
</ul></li>
</ul>
</section>

<section id="xgboost" class="title-slide slide level2">
<h2>XGBoost</h2>
<ul>
<li class="fragment">Explicit regularization in loss function</li>
<li class="fragment">Newton boosting:
<ul>
<li class="fragment"><span class="math inline">\(\mathbf{h}_m\)</span> is <span class="math display">\[-\rho_m \mathbf{H}_m^{-1}\mathbf{g}_m\]</span></li>
</ul></li>
<li class="fragment">Custom split point identification: “Weighted Quantile Sketch”</li>
<li class="fragment">Computationally optimized</li>
</ul>
</section>

<section id="learn-more" class="title-slide slide level2">
<h2>Learn more</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p><a href="https://web.stanford.edu/~hastie/ElemStatLearn/">Elements of Statistical Learning</a></p>
<p><img data-src="./../static/esl-cover.jpg" height="400" /></p>
</div><div class="column" style="width:50%;">
<p><a href="https://youtu.be/wPqtzj5VZus">Trevor Hastie - Gradient Boosting Talk (2014)</a></p>
<p><img data-src="./../static/trevor-hastie-gradient-boosting-thumbnail.jpg" /></p>
</div>
</div>
</section>

<section id="references" class="title-slide slide level2 allowframebreaks">
<h2 class="allowframebreaks">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-chen2015higgs" class="csl-entry" role="doc-biblioentry">
Chen, Tianqi, and Tong He. 2015. <span>“Higgs Boson Discovery with Boosted Trees.”</span> In <em>NIPS 2014 Workshop on High-Energy Physics and Machine Learning</em>, 69–80. PMLR.
</div>
<div id="ref-freund1990majority" class="csl-entry" role="doc-biblioentry">
Freund, Yoav. 1990. <span>“Boosting a Weak Learning Algorithm by Majority.”</span> In <em>Proceedings of the Third Annual Workshop on Computational Learning Theory, 1990</em>.
</div>
<div id="ref-friedman2001greedy" class="csl-entry" role="doc-biblioentry">
Friedman, Jerome H. 2001. <span>“Greedy Function Approximation: A Gradient Boosting Machine.”</span> <em>Annals of Statistics</em>, 1189–1232.
</div>
<div id="ref-friedman2000special" class="csl-entry" role="doc-biblioentry">
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2000. <span>“Special Invited Paper. Additive Logistic Regression: A Statistical View of Boosting.”</span> <em>Annals of Statistics</em>, 337–74.
</div>
<div id="ref-friedman2009elements" class="csl-entry" role="doc-biblioentry">
Friedman, Jerome, Trevor Hastie, Robert Tibshirani, et al. 2009. <em>The Elements of Statistical Learning</em>. 2nd ed. New York, NY: Springer.
</div>
<div id="ref-kearns1989crytographic" class="csl-entry" role="doc-biblioentry">
Kearns, M, and LG Valiant. 1989. <span>“Crytographic Limitations on Learning Boolean Formulae and Finite Automata.”</span> In <em>Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing</em>, 433–44.
</div>
<div id="ref-mason1999boosting" class="csl-entry" role="doc-biblioentry">
Mason, Llew, Jonathan Baxter, Peter Bartlett, and Marcus Frean. 1999. <span>“Boosting Algorithms as Gradient Descent in Function Space.”</span> In <em>Proc. NIPS</em>, 12:512–18.
</div>
<div id="ref-schapire1995decision" class="csl-entry" role="doc-biblioentry">
Schapire, R, and Y Freund. 1995. <span>“A Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.”</span> In <em>Second European Conference on Computational Learning Theory</em>, 23–37.
</div>
<div id="ref-schapire1990strength" class="csl-entry" role="doc-biblioentry">
Schapire, Robert E. 1990. <span>“The Strength of Weak Learnability.”</span> <em>Machine Learning</em> 5 (2): 197–227.
</div>
<div id="ref-valiant1984theory" class="csl-entry" role="doc-biblioentry">
Valiant, Leslie G. 1984. <span>“A Theory of the Learnable.”</span> <em>Communications of the ACM</em> 27 (11): 1134–42.
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^4//dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="https://unpkg.com/reveal.js@^4//plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/zoom/zoom.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'linear',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
