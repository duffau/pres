<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="author" content="Christian Duffau-Rasmussen">
  <title>Boosting</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }
  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/theme/black.css" id="theme">
  <link rel="stylesheet" href="../black-root.css"/>
</head>
<body>
  <div class="reveal">
    <div class="slides">

<section id="title-slide">
  <h1 class="title">Boosting</h1>
  <p class="author">Christian Duffau-Rasmussen</p>
  <p class="date">03-10-2021</p>
</section>

<section id="boosting---what-is-it" class="slide level2">
<h2>Boosting - What is it ?</h2>
<blockquote>
<p>A procedure to combine many <strong>weak</strong> learners to produce a powerful <strong>committee</strong>.</p>
<p><span class="citation" data-cites="friedman2009elements">(J. Friedman et al. 2009, sec. 10.1)</span></p>
</blockquote>
<aside class="notes">
<ul>
<li>An ensemble method in Machine Learning</li>
<li>A way of combining so-called weak learner</li>
<li>into a powerful comittee i.e.¬†by some means og aggregation or averaging</li>
</ul>
</aside>
</section>
<section id="a-brief-history" class="slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/leslie-valiant.jpg" height="130" /></p>
<p><span class="citation" data-cites="valiant1984theory">(Valiant 1984)</span></p>
<ul>
<li class="fragment">Probably Approximately Correct Learner (PAC)<br />
</li>
<li class="fragment">A formal theory of learnability</li>
<li class="fragment">Proof of some broad classes of non-trivial boolean functions are learnable</li>
</ul>
</div>
</div>
<aside class="notes">
<ul>
<li>Valiant (1984) - A theory of the learnable</li>
<li>Defines a mathematical framework for analyzing what classes of problems are learnable in polynomial time.</li>
<li>Introduces the Probably Approximately Correct Learner (PAC-learner).</li>
<li>Foundation of the field of computational learning theory.</li>
</ul>
</aside>
</section>
<section id="side-note-pac-learning" class="slide level2">
<h2>Side note: PAC learning</h2>
<ul>
<li class="fragment">A <em>concept</em> is Probably Approximately Learnable
<ul>
<li class="fragment"><em>if</em> in polynomial time an <em>algorithm</em> can deduce a <em>hypothesis</em></li>
<li class="fragment">with an error-rate <span class="math inline">\(&lt;\varepsilon\)</span> with probability at least <span class="math inline">\(1-\delta\)</span></li>
<li class="fragment"><span class="math inline">\(s\)</span> is the size of the <em>concept</em> in bits, encoded appropriately</li>
<li class="fragment"><em>for all</em> <span class="math inline">\(\varepsilon&gt;0\)</span> and <span class="math inline">\(\delta \leq 1\)</span></li>
</ul></li>
<li class="fragment">The <em>learning protocol</em> is:
<ul>
<li class="fragment">learn from examples asking an ORACLE</li>
<li class="fragment">the ORACLE returns a random example in <em>unit</em> time (O(1))</li>
</ul></li>
</ul>
</section>
<section id="a-brief-history-1" class="slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/leslie-valiant.jpg" height="130" alt="Leslie Valiant" /> <img data-src="../static/michael-kearns.jpg" height="130" alt="Michael Kearns" /></p>
<p><span class="citation" data-cites="kearns1989crytographic">(Kearns and Valiant 1989)</span></p>
<ul>
<li class="fragment">Introduce <em>weak learner</em><br />
</li>
<li class="fragment">Performs only slightly better than chance</li>
<li class="fragment"><em>Hypothesis boosting problem</em>: weak &lt;=&gt; strong ?</li>
</ul>
</div>
</div>
<aside class="notes">
<ul>
<li><p>The 1989 Crypto paper show that</p></li>
<li><p>If either:</p>
<ul>
<li>Boolean formulae</li>
<li>Deterministic finite automata</li>
<li>Constant-depth threshold circuits</li>
</ul></li>
<li><p>are learnable cryptography is toast.</p></li>
<li><p>Kearns and Valiant state as an open problem:</p>
<ul>
<li>Can weak learners be ‚Äúboosted‚Äù into strong learners?</li>
<li>I.e. can an algorithm transform weak leaners in to strong ones</li>
<li>The notion at the the time was ‚Äúprobably not‚Äù</li>
</ul></li>
</ul>
</aside>
</section>
<section id="side-note-weak-and-strong-learners" class="slide level2">
<h2>Side note: <em>weak</em> and <em>strong</em> learners</h2>
<ul>
<li class="fragment"><p><em>strongly learnable</em> == PAC learnable</p></li>
<li class="fragment"><p>A <em>concept</em> is <em>weakly</em> Learnable</p>
<ul>
<li class="fragment"><em>if</em> in polynomial time an <em>algorithm</em> can deduce a <em>hypothesis</em></li>
<li class="fragment">with an error-rate <span class="math inline">\(&lt;\frac{1}{2} - 1/p(s)\)</span> with probability at least <span class="math inline">\(1-\delta\)</span></li>
<li class="fragment"><em>for all</em> <span class="math inline">\(0 &lt; \delta \leq 1\)</span></li>
<li class="fragment">where <span class="math inline">\(s\)</span> is the size of the <em>concept</em> in bits, encoded appropriately</li>
</ul></li>
</ul>
</section>
<section id="a-brief-history-2" class="slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/robert-schapire.jpg" height="130" /></p>
<p><span class="citation" data-cites="schapire1990strength">(R. E. Schapire 1990)</span></p>
<ul>
<li class="fragment">Cracks the <em>Hypothesis boosting problem</em></li>
<li class="fragment">Shows <em>weak learner</em> &lt;=&gt; <em>strong learner</em></li>
<li class="fragment">An algorithm constructing a strong learner from weak ones ü§Ø</li>
</ul>
</div>
</div>
</section>
<section id="a-brief-history-3" class="slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/yoav-freund.png" height="130" /></p>
<p><span class="citation" data-cites="freund1990majority">(Freund 1990)</span></p>
<ul>
<li class="fragment">Implements a much more efficient <em>boosting</em> algorithm</li>
<li class="fragment">Trains learners on weighted subsets of the data</li>
<li class="fragment">Uses majority voting to predict</li>
</ul>
</div>
</div>
</section>
<section id="a-brief-history-4" class="slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/yoav-freund.png" height="130" /> <img data-src="../static/robert-schapire.jpg" height="130" /></p>
<p><span class="citation" data-cites="schapire1995decision">(R. Schapire and Freund 1995)</span></p>
<ul>
<li class="fragment">Introduces AdaBoost</li>
<li class="fragment">First practical boosting algorithm<br />
</li>
<li class="fragment">Has been very effective</li>
</ul>
</div>
</div>
</section>
<section id="a-brief-history-5" class="slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/jerome-h-friedman.jpeg" height="130" /> <img data-src="../static/robert-tibshirani-trevor-hastie.jpg" height="130" /></p>
<p><span class="citation" data-cites="friedman2000special">(J. Friedman, Hastie, and Tibshirani 2000)</span></p>
<ul>
<li class="fragment">Shows AdaBoost is <em>Stagewise Additive Logistic reg.</em></li>
</ul>
</div>
</div>
</section>
<section id="a-brief-history-6" class="slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/jerome-h-friedman.jpeg" height="130" /></p>
<p><span class="citation" data-cites="friedman2001greedy">(J. H. Friedman 2001)</span> <span class="citation" data-cites="mason1999boosting">(Mason et al. 1999)</span></p>
<ul>
<li class="fragment">Generalizes the <em>boosting</em> concept</li>
<li class="fragment">Describes <em>boosting</em> as gradient descent in function space</li>
</ul>
</div>
</div>
</section>
<section id="a-brief-history-7" class="slide level2">
<h2>A brief history</h2>
<div class="columns">
<div class="column" style="width:66%;">
<p><img data-src="../timeline/boosting_timeline.svg" width="1200" /></p>
</div><div class="column" style="width:33%;">
<p><img data-src="../static/tianqi-chen.jpg" height="130" /></p>
<p><span class="citation" data-cites="chen2015higgs">(Chen and He 2015)</span></p>
<ul>
<li class="fragment">Wins Kaggle contest on Higgs Boson using XGBoost</li>
<li class="fragment">XGBoost quickly becomes the most winning algorithm</li>
</ul>
</div>
</div>
</section>
<section id="ensemble-methods" class="slide level2">
<h2>Ensemble methods</h2>
<ul>
<li class="fragment">Bagging: Grow trees using <em>random subsets of data (with replacement)</em></li>
<li class="fragment">Random forest: Grow trees using <em>random subset of features</em></li>
<li class="fragment">Boosting: Grow trees on <em>re-weighted dataset</em></li>
</ul>
<div class="fragment">
<p><span class="math display">\[\text{Boosting} \succ \text{Random forest} \succ \text{Bagging} \succ \text{Tree}\]</span></p>
</div>
</section>
<section id="simulation-example" class="slide level2">
<h2>Simulation example</h2>
<p><span class="math display">\[Y = \begin{cases}
1 &amp; \text{if}\quad X_1^2 + \ldots + X_{10}^2 &gt; 9.34 \\
-1 &amp; \text{else}
\end{cases}\quad X_i\sim N(0,1)\]</span></p>
<figure>
<img data-src="simulation/gen_data.svg" height="400" alt="Simulated 10-D nested spheres" /><figcaption aria-hidden="true">Simulated 10-D nested spheres</figcaption>
</figure>
</section>
<section id="simulation-example-1" class="slide level2">
<h2>Simulation example</h2>
<p><img data-src="simulation/ensemble_test_errors.svg" /></p>
</section>
<section id="biasvariance-trade-off" class="slide level2">
<h2>Bias/variance trade-off</h2>
<p><span class="math display">\[\text{MSE} = \text{Bias}(\hat{f})^2 + \text{Var}(\hat{f}) + \text{Irreducible noise}\]</span></p>
</section>
<section id="variance-of-averages" class="slide level2">
<h2>Variance of averages</h2>
<p><span class="math display">\[\text{Var}\left(\frac{1}{n}\sum_i^n X_i\right) = \frac{1}{n}\sum_i^n \text{Var}\left(X_i\right) + \frac{1}{n}\sum_{i\neq j} \text{Cov}(X_i, X_j)\]</span></p>
<div class="fragment">
<p><span class="math display">\[\text{Variance of ensemble} = \frac{\text{Var(Trees)}}{n} + \frac{\text{Cov( Trees)}}{n}\]</span></p>
</div>
</section>
<section id="variance-and-bias-reduction" class="slide level2">
<h2>Variance and bias reduction</h2>
<figure>
<img data-src="simulation/consecutive_predictions_corr.svg" alt="Classifying 10-D nested spheres" /><figcaption aria-hidden="true">Classifying 10-D nested spheres</figcaption>
</figure>
</section>
<section id="boosting" class="slide level2">
<h2>Boosting</h2>
<p><img data-src="../static/Ensemble_Boosting.svg" /></p>
<aside class="notes">
<ul>
<li>A collection of <em>weak learners</em> (e.g.¬†classifier) are trained sequentially.</li>
<li>Each <em>learner</em> is trained on the same dataset.</li>
<li>Each example is re-weighted in each iteration</li>
<li>Poorly predicted examples get higher weight</li>
<li>Well predicted examples get lower weight</li>
</ul>
</aside>
</section>
<section id="forward-stagewise-additive-learning" class="slide level2">
<h2>Forward Stagewise Additive Learning</h2>
<ol type="1">
<li class="fragment">Initialize <span class="math inline">\(f_0(x) = 0\)</span></li>
<li class="fragment">For <span class="math inline">\(m=1\)</span> to <span class="math inline">\(M\)</span>:
<ol type="a">
<li class="fragment">Compute <span class="math display">\[(\beta_m, \gamma_m) = \underset{\beta, \gamma}{\text{argmin}} \sum_{i=1}^N L(y_i, f_{m-1}(x_i)+ \beta b(x_i;\gamma))\]</span></li>
<li class="fragment">Set <span class="math inline">\(f_m(x) = f_{m-1}(x) + \beta_m b(x; \gamma_m)\)</span></li>
</ol></li>
</ol>
<aside class="notes">
<ul>
<li>Boosting is a special case of <em>Forward Stagewise Additive Learning</em></li>
<li>It‚Äôs an approximation technique for general additive models</li>
</ul>
</aside>
</section>
<section id="adaboost" class="slide level2">
<h2>Adaboost</h2>
<ol type="1">
<li class="fragment">Initialize weights <span class="math inline">\(w_i=1/N\)</span></li>
<li class="fragment">For <span class="math inline">\(m=1\)</span> to <span class="math inline">\(M\)</span>:
<ol type="1">
<li class="fragment">Fit classifier <span class="math inline">\(G_m(x)\)</span> to training data using <span class="math inline">\(w_i\)</span>‚Äôs</li>
<li class="fragment">Compute <span class="math display">\[\text{err} = \frac{\sum_{i=1}^N w_i I(y_i \neq G_m(x_i))}{\sum_{i=1}^N w_i}\]</span></li>
<li class="fragment">Compute <span class="math inline">\(\alpha_m = \log((1-\text{err})/\text{err})\)</span></li>
<li class="fragment">Set <span class="math inline">\(w_i \leftarrow \exp(\alpha_m I(y_i \neq G_m(x_i)))\)</span></li>
</ol></li>
<li class="fragment">Output <span class="math display">\[G(x) = \text{sign}\left(\sum_{m=1}^M \alpha_m G_m(x)\right)\]</span></li>
</ol>
</section>
<section id="adaboost-1" class="slide level2">
<h2>Adaboost</h2>
<ul>
<li class="fragment">Not originally motivated in <em>Forward Stagewise Additive Learning</em></li>
<li class="fragment">One can show that Adaboost is a stagewise additive model with loss <span class="math display">\[L(y, f (x)) = \exp(‚àíy f(x))\]</span> where <span class="math inline">\(y\in \{-1, 1\}\)</span> and <span class="math inline">\(f(x) \in \mathbb{R}\)</span>.</li>
<li class="fragment">Usually for classification we use cross-entropy <span class="math display">\[L(y, f(x)) = y^\prime \log p(x) + (1-y^\prime) \log(1- p(x)) \]</span> where <span class="math inline">\(y^\prime = (y+1)/2\in \{0,1\}\)</span> and <span class="math inline">\(p(x)\)</span> is the softmax function.</li>
<li class="fragment">In theory the two loss functions are equivalent</li>
<li class="fragment">On average they produce the same <em>model</em> <span class="math inline">\(f\)</span></li>
</ul>
</section>
<section id="adaboost-2" class="slide level2">
<h2>Adaboost</h2>
<ul>
<li class="fragment">For finite samples the exponential loss has drawbacks</li>
<li class="fragment">To much weight is given to errors</li>
</ul>
<div class="fragment">
<figure>
<img data-src="plots/loss_functions.svg" height="300" alt="Exponential loss and cross-entropy" /><figcaption aria-hidden="true">Exponential loss and cross-entropy</figcaption>
</figure>
</div>
</section>
<section id="adaboost-3" class="slide level2">
<h2>Adaboost</h2>
<ul>
<li class="fragment">10D nested spheres with noisy data</li>
</ul>
<div class="columns">
<div class="column" style="width:50%;">
<figure>
<img data-src="simulation/gen_data.svg" alt="Clean data" /><figcaption aria-hidden="true">Clean data</figcaption>
</figure>
</div><div class="column" style="width:50%;">
<figure>
<img data-src="simulation/gen_noisy_data.svg" alt="Noisy data" /><figcaption aria-hidden="true">Noisy data</figcaption>
</figure>
</div>
</div>
</section>
<section id="adaboost-4" class="slide level2">
<h2>Adaboost</h2>
<ul>
<li class="fragment">Adaboost has trouble on noisy data</li>
</ul>
<div class="fragment">
<div class="columns">
<div class="column" style="width:50%;">
<figure>
<img data-src="simulation/ensemble_test_errors.svg" alt="Test error clean data" /><figcaption aria-hidden="true">Test error clean data</figcaption>
</figure>
</div><div class="column" style="width:50%;">
<figure>
<img data-src="simulation/ensemble_test_errors_noisy.svg" alt="Test error noisy data" /><figcaption aria-hidden="true">Test error noisy data</figcaption>
</figure>
</div>
</div>
<aside class="notes">
<ul>
<li>The deviance yf is only negative if they have different signs</li>
<li>So all negative values are errors</li>
<li>The exponential loss eight these error much heavier</li>
</ul>
</aside>
</div>
</section>
<section id="adaboost-5" class="slide level2">
<h2>Adaboost</h2>
<ul>
<li class="fragment">How to fix Adaboost?</li>
<li class="fragment">Can we easily plug in a more robust loss function?</li>
</ul>
<div class="fragment">
<div>
<ol type="1">
<li>Initialize weights <span class="math inline">\(w_i=1/N\)</span></li>
<li>For <span class="math inline">\(m=1\)</span> to <span class="math inline">\(M\)</span>:</li>
<li>Fit classifier <span class="math inline">\(G_m(x)\)</span> to training data using <span class="math inline">\(w_i\)</span>‚Äôs</li>
<li>Compute <span class="math inline">\(\text{err} = \frac{\sum_{i=1}^N w_i I(y_i \neq G_m(x_i))}{\sum_{i=1}^N w_i}\)</span></li>
<li>Compute <span class="math inline">\(\alpha_m = \log((1-\text{err})/\text{err})\)</span></li>
<li>Set <span class="math inline">\(w_i \leftarrow \exp(\alpha_m I(y_i \neq G_m(x_i)))\)</span></li>
<li>Output <span class="math display">\[G(x) = \text{sign}\left(\sum_{m=1}^M \alpha_m G_m(x)\right)\]</span></li>
</ol>
</div>
<p>ü§∑‚Äç‚ôÇÔ∏è</p>
</div>
</section>
<section id="gradient-boosting" class="slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">Introduced by <span class="citation" data-cites="friedman2001greedy">(J. H. Friedman 2001)</span> as a new view of boosting</li>
<li class="fragment">Makes it possible to <em>derive</em> a boosting procedure</li>
<li class="fragment">Solves each forward stagewise step as <em>gradient descent</em> in a function space</li>
<li class="fragment">Applies to any differentiable loss function</li>
</ul>
</section>
<section id="side-note-gradient-boosting-in-scikit-learn" class="slide level2">
<h2>Side note: Gradient boosting in SciKit Learn</h2>
<p><em>Scikit-learn documentation</em>:</p>
<blockquote>
<p>GB builds an additive model in a forward stage-wise fashion; it allows for the optimization of arbitrary differentiable loss functions.</p>
</blockquote>
<div class="fragment">
<p>Sound promising‚Ä¶</p>
</div>
<div class="fragment">
<blockquote>
<p><code>loss: {‚Äòdeviance‚Äô, ‚Äòexponential‚Äô}</code></p>
<p>For loss ‚Äòexponential‚Äô gradient boosting recovers the AdaBoost algorithm.</p>
</blockquote>
</div>
<div class="fragment">
<p>ü§¶‚Äç‚ôÇÔ∏è</p>
</div>
</section>
<section id="gradient-boosting-1" class="slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">Consider the loss function, <span class="math display">\[\sum_{i=1}^N L\left(y_i, \sum_{m=1}^M \beta_m b(x_i, \gamma_m)\right)\]</span></li>
<li class="fragment">‚Ä¶ to be optimized over <span class="math inline">\(\{\beta_m, \gamma_m\}_{m=1}^M\)</span></li>
<li class="fragment">‚Ä¶ which might me intractable</li>
</ul>
</section>
<section id="gradient-boosting-2" class="slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">We try to solve it in a Forward stagewise mamner</li>
<li class="fragment">For <span class="math inline">\(1\)</span> to <span class="math inline">\(M\)</span>:
<ul>
<li class="fragment">Optimize <span class="math display">\[\sum_{i=1}^N L\left(y_i, f_{m-1}(x_i) + \beta_m b(x_i, \gamma_m)\right)\]</span></li>
</ul></li>
<li class="fragment">where <span class="math inline">\(f_m(x) = f_{m-1}(x) + \beta_m b(x_i, \gamma_m)\)</span>.</li>
<li class="fragment">If the basis functions are trees</li>
<li class="fragment">‚Ä¶ the stagewise minimization ca be difficult</li>
</ul>
</section>
<section id="gradient-boosting-3" class="slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">Consider the expected loss function <span class="math display">\[\phi(f) = E[L(y,f(\boldsymbol{x}))]\]</span></li>
<li class="fragment">we seek <span class="math inline">\(f\)</span> that minimizes <span class="math inline">\(\phi\)</span></li>
<li class="fragment">Thats a difficult variational analysis problem</li>
<li class="fragment">Let‚Äôs circumvent the problem‚Ä¶</li>
<li class="fragment">‚Ä¶ consider a dataset of <span class="math inline">\(N\)</span> observations instead <span class="math display">\[\mathbf{f} = \{f(x_1), \ldots, f(x_N)\}\]</span></li>
</ul>
</section>
<section id="gradient-boosting-4" class="slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">Lets minimize the empirical loss <span class="math display">\[L(f) = \sum_{i=1}^N L(y_i, f(x_i))\]</span> wrt. <span class="math inline">\(f(x_i)\)</span> numerically.</li>
<li class="fragment">All numerical optimization is a series of <span class="math inline">\(M\)</span> steps towards optimum</li>
<li class="fragment">Let each step be a vector <span class="math inline">\(\mathbf{h}_m\in\mathbb{R}^N\)</span></li>
<li class="fragment">The numerical optimum is <span class="math display">\[\mathbf{f}_M = \sum_{m=1}^M \mathbf{h}_m\]</span></li>
</ul>
</section>
<section id="gradient-boosting-5" class="slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">In <em>gradient descent</em> each step <span class="math inline">\(\mathbf{h}_m\)</span> is <span class="math display">\[-\rho \mathbf{g}_m\]</span></li>
<li class="fragment">‚Ä¶ where <span class="math inline">\(g_{im}\)</span> is <span class="math display">\[\left[\frac{\partial L(y_i, f(x_i))}{\partial  f(x_i)}\right]_{f(x)=f_{m-1}(x)}\]</span></li>
<li class="fragment">The numerical optimization is then <span class="math display">\[
\begin{align*}
\mathbf{f}_m &amp;= \mathbf{f}_{m-1} + \mathbf{h}_m\\
&amp;= \mathbf{f}_{m-1} -\rho \mathbf{g}_m 
\end{align*}
\]</span></li>
</ul>
</section>
<section id="gradient-boosting-6" class="slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">Let‚Äôs add <em>line search</em>, <span class="math display">\[\mathbf{f}_m = \mathbf{f}_{m-1} - \rho_m \mathbf{g}_m\]</span> where <span class="math inline">\(\rho_m = \underset{\rho}{\text{argmin}}\quad L(\mathbf{f}_{m-1}-\rho \mathbf{g}_m )\)</span></li>
<li class="fragment">This fits the pattern of <em>forward stagewise learning</em> <span class="math display">\[f_m(x) = f_{m-1}(x) + \beta_m b(x; \gamma_m)\]</span></li>
<li class="fragment">where <span class="math inline">\(\rho_m\)</span> is analogous to <span class="math inline">\(\beta_m\)</span></li>
<li class="fragment">and <span class="math inline">\(-\mathbf{g}_m\)</span> is analogous to <span class="math inline">\(b(x; \gamma_m)\)</span></li>
<li class="fragment">We can derive the gradient considering <span class="math inline">\(f(x)\)</span> a variable</li>
<li class="fragment">‚Ä¶ and easily compute <span class="math inline">\(\mathbf{g}_m(f(x))\)</span> using the value from the previous iteration <span class="math inline">\(f_{m-1}(x)\)</span></li>
<li class="fragment">‚Ä¶ we cannot compute <span class="math inline">\(\mathbf{g}_m\)</span> outside the dataset ü§î</li>
</ul>
</section>
<section id="gradient-boosting-7" class="slide level2">
<h2>Gradient boosting</h2>
<ul>
<li class="fragment">The idea of <span class="citation" data-cites="friedman2001greedy">(J. H. Friedman 2001)</span>:
<ul>
<li class="fragment">Fit a <em>learner</em> to <span class="math inline">\(-\mathbf{g}_m\)</span> <span class="math inline">\(\rightarrow \hat{b}_m(x; \gamma_m)\)</span></li>
<li class="fragment">Forward stagewise step: <span class="math display">\[f_m(x) = f_{m-1}(x) + \rho_m \hat{b}_m(x; \gamma_m)\]</span></li>
</ul></li>
</ul>
</section>
<section id="xgboost" class="slide level2">
<h2>XGBoost</h2>
<ul>
<li class="fragment">Explicit regularization in loss function</li>
<li class="fragment">Newton boosting:
<ul>
<li class="fragment"><span class="math inline">\(\mathbf{h}_m\)</span> is <span class="math display">\[-\rho_m \mathbf{H}_m^{-1}\mathbf{g}_m\]</span></li>
</ul></li>
<li class="fragment">Custom split point identification: ‚ÄúWeighted Quantile Sketch‚Äù</li>
<li class="fragment">Computationally optimized</li>
</ul>
</section>
<section id="references" class="slide level2 allowframebreaks">
<h2 class="allowframebreaks">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-chen2015higgs" class="csl-entry" role="doc-biblioentry">
Chen, Tianqi, and Tong He. 2015. <span>‚ÄúHiggs Boson Discovery with Boosted Trees.‚Äù</span> In <em>NIPS 2014 Workshop on High-Energy Physics and Machine Learning</em>, 69‚Äì80. PMLR.
</div>
<div id="ref-freund1990majority" class="csl-entry" role="doc-biblioentry">
Freund, Yoav. 1990. <span>‚ÄúBoosting a Weak Learning Algorithm by Majority.‚Äù</span> In <em>Proceedings of the Third Annual Workshop on Computational Learning Theory, 1990</em>.
</div>
<div id="ref-friedman2001greedy" class="csl-entry" role="doc-biblioentry">
Friedman, Jerome H. 2001. <span>‚ÄúGreedy Function Approximation: A Gradient Boosting Machine.‚Äù</span> <em>Annals of Statistics</em>, 1189‚Äì1232.
</div>
<div id="ref-friedman2000special" class="csl-entry" role="doc-biblioentry">
Friedman, Jerome, Trevor Hastie, and Robert Tibshirani. 2000. <span>‚ÄúSpecial Invited Paper. Additive Logistic Regression: A Statistical View of Boosting.‚Äù</span> <em>Annals of Statistics</em>, 337‚Äì74.
</div>
<div id="ref-friedman2009elements" class="csl-entry" role="doc-biblioentry">
Friedman, Jerome, Trevor Hastie, Robert Tibshirani, et al. 2009. <em>The Elements of Statistical Learning</em>. 2nd ed. New York, NY: Springer.
</div>
<div id="ref-kearns1989crytographic" class="csl-entry" role="doc-biblioentry">
Kearns, M, and LG Valiant. 1989. <span>‚ÄúCrytographic Limitations on Learning Boolean Formulae and Finite Automata.‚Äù</span> In <em>Proceedings of the Twenty-First Annual ACM Symposium on Theory of Computing</em>, 433‚Äì44.
</div>
<div id="ref-mason1999boosting" class="csl-entry" role="doc-biblioentry">
Mason, Llew, Jonathan Baxter, Peter Bartlett, and Marcus Frean. 1999. <span>‚ÄúBoosting Algorithms as Gradient Descent in Function Space.‚Äù</span> In <em>Proc. NIPS</em>, 12:512‚Äì18.
</div>
<div id="ref-schapire1995decision" class="csl-entry" role="doc-biblioentry">
Schapire, R, and Y Freund. 1995. <span>‚ÄúA Decision-Theoretic Generalization of on-Line Learning and an Application to Boosting.‚Äù</span> In <em>Second European Conference on Computational Learning Theory</em>, 23‚Äì37.
</div>
<div id="ref-schapire1990strength" class="csl-entry" role="doc-biblioentry">
Schapire, Robert E. 1990. <span>‚ÄúThe Strength of Weak Learnability.‚Äù</span> <em>Machine Learning</em> 5 (2): 197‚Äì227.
</div>
<div id="ref-valiant1984theory" class="csl-entry" role="doc-biblioentry">
Valiant, Leslie G. 1984. <span>‚ÄúA Theory of the Learnable.‚Äù</span> <em>Communications of the ACM</em> 27 (11): 1134‚Äì42.
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^4//dist/reveal.js"></script>

  <!-- reveal.js plugins -->
  <script src="https://unpkg.com/reveal.js@^4//plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/zoom/zoom.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/math/math.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'linear',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    </body>
</html>
