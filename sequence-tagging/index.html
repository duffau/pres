<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2024-09-05">
  <title>A Blast from the Past</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/theme/black.css" id="theme">
  <link rel="stylesheet" href="static/custom.css"/>
  <link rel="stylesheet" href="static/custom-elements.css"/>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/vs2015.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
  <meta property="og:url" content="https://duffau.github.io/talks/sequence-tagging/">
  <meta property="og:type" content="website">
  <meta property="og:image" content="https://duffau.github.io/talks/sequence-tagging/static/crf.svg" />
  <meta property="og:title" content="CRF's a Blast from the Past" />
  <meta property="og:description" content="Conditional Random Field models, which where big in the early 2000's, are light weight fast when it comes to sequence tagging. In this talk we investigate how they stack up against classical Transformers and LLMs, both in terms of accuracy and speed.">
  <meta name="twitter:card" content="summary_large_image">
  <meta property="twitter:domain" content="duffau.github.io">
  <meta property="twitter:url" content="https://duffau.github.io/talks/sequence-tagging/">
  <meta name="twitter:title" content="CRF's a Blast from the Past">
  <meta name="twitter:description" content="Conditional Random Field models, which where big in the early 2000's, are light weight fast when it comes to sequence tagging. In this talk we investigate how they stack up against classical Transformers and LLMs, both in terms of accuracy and speed.">
  <meta name="twitter:image" content="https://duffau.github.io/talks/sequence-tagging/static/crf.svg">
</head>
<body>
<div class="watermark">
<p><a href="https://alipes.dk/" rel="noopener noreferrer"> <img
      src="static/alipes-logo.svg"
    /> </a></p>
</div>
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="./static/crf.svg" data-background-opacity="0.2" data-background-size="80%">
  <h1 class="title">A <em>Blast</em> from the <em>Past</em></h1>
  <p class="subtitle"><em>Fast</em> Text Tagging with Conditional Random
Fields</p>
  <p class="date">2024-09-05</p>
</section>

<section id="about-me" class="slide level3">
<h3>About Me</h3>
<div>
<ul>
<li class="fragment">Senior Machine Learning Scientist at Alipes
<ul>
<li class="fragment">News Trading Algorithms</li>
<li class="fragment">Applied NLP and Machine Learning</li>
</ul></li>
<li class="fragment">Msc. in Economics from University of Copenhagen
from 2016
<ul>
<li class="fragment">Fell into the “<span
class="math inline">\(Math\)</span> and <code>Code</code>” pot at
University</li>
</ul></li>
<li class="fragment">Before joining Alipes
<ul>
<li class="fragment">Automated Sports Betting</li>
<li class="fragment">Co-Founded a Machine Learning Consultancy</li>
<li class="fragment">Co-founded a NLP-powered Legal Tech start-up</li>
</ul></li>
</ul>
</div>
</section>
<section id="slides" class="slide level3">
<h3>Slides</h3>
<p><img data-src="./static/talk-url-qr-code.svg" style="width:40.0%" />
<a
href="https://duffau.github.io/talks/sequence-tagging">duffau.github.io/talks/sequence-tagging</a></p>
</section>
<section id="sequence-tagging" class="slide level3">
<h3>Sequence Tagging</h3>
<pre class="txt"><code>The price of the [Pizza Margherita] is [10 dollars]. 
                  FOOD                  AMOUNT</code></pre>
</section>
<section id="section" class="slide level3">
<h3></h3>
<h4 id="named-entity-recognition-ner">Named Entity Recognition
(NER)</h4>
<pre class="txt"><code>Jim   worked at    Acme Corp. near the beautiful London Bridge.
PER   O      O     ORG  ORG   O    O   O         LOC    LOC   EOS</code></pre>
<h4 id="part-of-speech-pos">Part-of-Speech (POS)</h4>
<pre class="txt"><code>Jim   worked at    Acme Corp. near the beautiful London Bridge.
NOUN  VERB   PREP  NOUN NOUN  PREP DET ADJ       NOUN   NOUN  EOS</code></pre>
<p><span class="math display">\[\begin{aligned}
\text{Labels}:\quad \mathbf{y} &amp;= \{y_1, y_2, \ldots, y_T\}\\
\text{Features}:\quad \mathbf{x} &amp;= \{\mathbf{x}_1, \mathbf{x}_2,
\ldots, \mathbf{x}_T\}
\end{aligned}\]</span></p>
</section>
<section>
<section id="evolution-of-nlp-and-sequence-tagging"
class="title-slide slide level2">
<h2>Evolution of NLP and Sequence tagging</h2>

</section>
<section id="ai-over-optimism-and-ai-winter" class="slide level3">
<h3>1954-1966 - AI Over-optimism and AI Winter</h3>
<div>
<ul>
<li class="fragment">1954: IBM-Georgetown machine translation: Sixty
Russian sentences translated into English</li>
<li class="fragment">1957: Noam Chomsky <em>Syntactic Structures</em>:
<ul>
<li class="fragment"><em>Generative Grammar</em>: A system of rules that
generate exactly those combinations of words that form grammatical
sentences</li>
<li class="fragment"><em>Anti-probabilistic</em>: “probabilistic models
give no particular insight into some of the basic problems of syntactic
structure.”</li>
</ul></li>
</ul>
</div>
</section>
<section id="ai-over-optimism-and-ai-winter-1" class="slide level3">
<h3>1954-1966 - AI Over-optimism and AI Winter</h3>
<div>
<ul>
<li class="fragment">1958: H. A. Simon and Allen Newell:“within ten
years a digital computer will discover and prove an important new
mathematical theorem”</li>
<li class="fragment">1960’s: Slow progress in machine translation</li>
<li class="fragment">1966: ALPAC report ledas to defunding of machine
translation in US</li>
<li class="fragment">1974–1980: First AI Winter</li>
</ul>
</div>
</section>
<section id="s---expert-systems" class="slide level3">
<h3>1980’s - Expert Systems</h3>
<div>
<ul>
<li class="fragment">Expert Systems
<ul>
<li class="fragment">Knowledge base - Collected daa</li>
<li class="fragment">Inference engine - Hand crafted expert rules</li>
</ul></li>
<li class="fragment">Examples:
<ul>
<li class="fragment">CADUCEUS: Medical expert system “most
knowledge-intensive expert system in existence”</li>
<li class="fragment">Dendral: Decision-making process and
problem-solving behavior of organic chemists.</li>
</ul></li>
</ul>
</div>
</section>
<section id="late-1980s-and-1990s---rise-of-statistical-models"
class="slide level3">
<h3>Late 1980’s and 1990’s - Rise of Statistical Models</h3>
<div>
<ul>
<li class="fragment">Less dominance of Chomskyan theories of
linguistics</li>
<li class="fragment">More computational power</li>
<li class="fragment">Availability of Annotated Datasets</li>
<li class="fragment">Give Rise to Statistical NLP</li>
</ul>
</div>
</section>
<section id="late-1980s-and-1990s---rise-statistical-models"
class="slide level3">
<h3>Late 1980’s and 1990’s - Rise Statistical Models</h3>
<div>
<ul>
<li class="fragment">1988: First papers using Markov models for PoS
<span class="citation"
data-cites="derose1988grammatical church1989stochastic">(DeRose 1988;
Church 1989)</span></li>
<li class="fragment">1989: Tutorial on Hidden Markov Models (HMM) for
Speech <span class="citation" data-cites="rabiner1989tutorial">(Rabiner
1989)</span></li>
<li class="fragment">1993: Penn Treebank Project</li>
<li class="fragment">1995: WordNet: A Lexical Database for English, 6th
Message Understanding Conference</li>
<li class="fragment">1996: Maximum Entropy Markov Model (MEMM) published
<span class="citation" data-cites="ratnaparkhi1996maximum">(Ratnaparkhi
1996)</span></li>
<li class="fragment">2001: Conditional Random Fields (CRF) <span
class="citation" data-cites="lafferty2001conditional">(Lafferty et al.
2001)</span></li>
</ul>
</div>
<aside class="notes">
<ul>
<li>HMM: The first to encode sequential information for PoS</li>
<li>Rabiner: HMM applied to speech since th 70’s but only widely know in
“recent years”
<ul>
<li>33,500 citation</li>
</ul></li>
<li>Penn Treebank Project: 1 mio annotated tokens from WSJ</li>
<li>MUC 6: https://aclanthology.org/volumes/M95-1/
<ul>
<li>Arranged by Naval Command and DARPA</li>
<li>A information extraction competition 10-20 participants from
industry adn universities 6 months work</li>
<li>Defined evalution methods and metrics like precision, recall and
f1</li>
<li>Task:
<ul>
<li>MUC-4: “Terrorist activities in Latin America”</li>
<li>MUC-6: “Negotiation of Labor Disputes”</li>
</ul></li>
<li>Coining the “NAmed Entity” at MUC-6</li>
</ul></li>
<li>MEMM:
<ul>
<li>Discriminative markov model - More direct model</li>
<li>Allows influence from features at any point in time</li>
</ul></li>
<li>Condtional Random Fields:
<ul>
<li>Sequential like HMM and MEMM</li>
<li>Are discriminative (like MEMM)</li>
<li>Allows influence from features at any point in time (Like MEMM)</li>
<li>Solves the “label bias” issue of MEMM’s</li>
</ul></li>
</ul>
</aside>
</section>
<section id="s-to-2010---large-corpus-from-the-web"
class="slide level3">
<h3>2000’s to 2010 - Large Corpus from the Web</h3>
<ul>
<li>Standardization on benchmarks e.g. CoNLL 2003 <span class="citation"
data-cites="sang2003introduction">(Sang and De Meulder 2003)</span></li>
<li>Large raw (unannotated) dataset from the Web</li>
<li>Semi-supervised and Unsupervised Learning Approaches</li>
<li>2003: Multilayer perceptron beats Markov model in “next word
prediction” <span class="citation"
data-cites="bengio2000neural">(Bengio, Ducharme, and Vincent
2000)</span></li>
<li>Support Vector Machines for NER tagging</li>
</ul>
</section>
<section id="to-today---neural-models-and-word-embeddings"
class="slide level3">
<h3>2010 to Today - Neural Models and Word Embeddings</h3>
<ul>
<li>2013-14: Word Embeddings (Word2Vec, GloVe) <span class="citation"
data-cites="mikolov2013efficient">Pennington, Socher, and Manning
(2014)</span></li>
<li>2015: Neural Net Revolution (BiLSTM-CRF <span class="citation"
data-cites="huang2015bidirectional">(Huang, Xu, and Yu
2015)</span>)</li>
<li>2017: “Attention is all you need” <span class="citation"
data-cites="vaswani2017attention">(Vaswani 2017)</span></li>
<li>2019: First gen Transformers (BERT, RoBERTa, Electra)</li>
<li>2020: Large-Scale Pre-trained Language Models (GPT-3)</li>
<li>2022: GPT 3.5 and ChatGPT</li>
<li>2023: Large Language Models and Few-Shot Adaptation (GPT-4, Claude,
LLaMA)</li>
</ul>
</section>
<section id="papers-using-datasets-1" class="slide level3">
<h3>Papers Using Datasets <sup>1</sup></h3>
<div style="height:400px">
<canvas data-chart="line">
<!--
{
 "data": {
  "labels": [2020,2021,2022,2023,2024],
  "datasets":[
   {
    "data":[86,88,43,41,18],
    "label":"POS - Penn Treebank",
    "yAxisID": "y",
    "fill": false
   },
   {
    "data":[63,112,105,156, 195],
    "label":"QA - TriviaQA (Wiki + Web)",
    "yAxisID": "y1"
   },
   {
    "data":[16,30,54,160,327],
    "label":"NLI - HellaSwag Sentence Completion",
    "yAxisID": "y1"
   }
  ]
 },
 "options": {
  "scales": {
   "y": {
    "type": "linear",
    "display": true,
    "position": "left",
    "title": {
     "display": true,
     "text": "Published Papers"
    }
   },
   "y1": {
    "type": "linear",
    "display": true,
    "position": "right",
    "title": {
     "display": true,
     "text": "Published Papers"
    },
    "grid": {
     "drawOnChartArea": false
    }
   }
  }
 }
}
-->
</canvas>
</div>
<p>Abstract tasks have taken over lower level tasks</p>
<div class="footer">
<p><sup>1</sup> Source: https://paperswithcode.com/datasets</p>
</div>
<aside class="notes">
<ul>
<li>Penn Tree Bank
<ul>
<li>Penn State Tree Bank</li>
<li>Initially released in 1992</li>
<li>First richly annotated text corpus</li>
<li>1 mio Annotated tokens (2500 stories) from Wall Street Journal
Article from 1989 Wall Street Journal</li>
<li>2022: Sequence Aligment Ensemble-BART encoder: 98.15 Accuracy
<ul>
<li>Ensemble of BART models</li>
<li>Weighted voting where weights a proportional to avg. alignment score
with other predictions in ensemble<br />
</li>
</ul></li>
<li>2018: BI-LSTM: 97.96 Accuracy</li>
</ul></li>
<li>TriviaQA: Challenging than QA pairs
<ul>
<li>Long context</li>
<li>Answers not optained by span prediction in question or context</li>
<li>2017 University of Washington NLP</li>
<li>Claude 5 shots: 87.5 f1 score</li>
<li>https://paperswithcode.com/sota/question-answering-on-triviaqa</li>
</ul></li>
<li>HellaSwag: Common sense Natural Language Inference
<ul>
<li>“A woman sits at a piano,” -&gt; “She sets her fingers on the
keys.”</li>
<li>Humans have 95% accuracy</li>
<li>From Allen Institute for AI a Non-Profit research org.</li>
<li>GPT4 10 shots: 95.3 Accuracy</li>
<li>https://paperswithcode.com/sota/sentence-completion-on-hellaswag</li>
</ul></li>
</ul>
</aside>
</section>
<section id="ner-tagging-approaches" class="slide level3">
<h3>NER Tagging Approaches</h3>
<p><img data-src="static/ner-approaches.png" /></p>
</section></section>
<section>
<section id="crf-training-demo" class="title-slide slide level2">
<h2>CRF Training Demo</h2>

</section>
<section id="section-1" class="slide level3">
<h3></h3>
<pre class="python"><code>def tokens_to_features(tokens, i):
    features = {
      &quot;bias&quot;: 1.0,
      &quot;word&quot;: tokens[i].lower(),
      &quot;prev_word&quot;: tokens[i-1],
      &quot;next_word&quot;: tokens[i+1],
      &quot;shape&quot;: re.sub(&quot;X&quot;, &quot;\d&quot;, word)
    }
    return features</code></pre>
</section>
<section class="slide level3">

<pre class="python"><code>import datasets

def load_X_y(dataset_id=&quot;eriktks/conll2003&quot;, split=&quot;train&quot;):
    data = datasets.load_dataset(dataset_id)
    sentences = data[split][&quot;tokens&quot;]
    labels = data[split][&quot;ner_tags&quot;]
    label_names = data[split].features[&quot;ner_tags&quot;].feature.names

    X, y = [], []
    for (sentence, label_seq), i in enumerate(sentences, labels):
      X.append(tokens_to_features(tokens, i)) 
      y.append([label_names[label_id] in label_seq])
    return X, y</code></pre>
</section>
<section class="slide level3">

<pre class="python"><code>import sklearn_crfsuite
from sklearn_crfsuite import metrics

X, y = load_X_y(split=&quot;train&quot;)
crf = sklearn_crfsuite.CRF(
  algorithm=&#39;lbfgs&#39;,
  c1=0.5, 
  c2=0.01,
)
crf.fit(X, y)</code></pre>
<pre class="bash"><code>Iter 1   time=0.33  loss=232365.88 active=86384 feature_norm=1.00
Iter 2   time=0.17  loss=217017.29 active=84206 feature_norm=3.45
Iter 3   time=0.17  loss=161378.43 active=83724 feature_norm=2.99
...
Iter  99 time=0.17  loss=1255.7  active=38787 feature_norm=252.98
Iter 100 time=0.16  loss=1255.7  active=38790 feature_norm=252.96
Total seconds required for training: 23.856</code></pre>
</section>
<section class="slide level3">

<pre><code>Number of active features: 38790 (86687)
Number of active attributes: 24451 (68166)
Number of active labels: 9 (9)</code></pre>
<pre class="python"><code># Worst case (all_possible_states=True and all_possible_transitions=True)
features = (number of attributes * number of labels) 
          + (number of labels * number of labels)</code></pre>
</section>
<section class="slide level3">

<pre><code>X_test, y_test = load_X_y(split=&quot;test&quot;)
y_pred = crf.predict(X_test)
print(metrics.flat_classification_report(y_test, y_pred))</code></pre>
<pre class="bash"><code>              precision    recall  f1-score   support

       B-LOC      0.852     0.837     0.844      1668
       I-LOC      0.741     0.623     0.677       257
      B-MISC      0.796     0.758     0.777       702
      I-MISC      0.647     0.653     0.650       216
       B-ORG      0.778     0.722     0.749      1661
       I-ORG      0.666     0.734     0.699       835
       B-PER      0.839     0.853     0.846      1617
       I-PER      0.879     0.952     0.914      1156

   micro avg      0.805     0.804     0.805      8112
   macro avg      0.775     0.766     0.769      8112
weighted avg      0.805     0.804     0.804      8112</code></pre>
</section>
<section id="typical-features" class="slide level3">
<h3>Typical features</h3>
<ul>
<li><code>word</code>: one-hot encoding of current word</li>
<li><code>+1:word</code>: one-hot encoding of next word</li>
<li><code>word length</code></li>
<li><code>word shape</code>: <code>$1,230</code> -&gt;
<code>$x,xxx</code></li>
<li><code>word.isupper()</code></li>
<li>POS tag</li>
<li>Adjacent bi-grams adn tri-grams</li>
<li>Windows of words</li>
<li>Gazetteers</li>
</ul>
</section></section>
<section>
<section id="crf-theory" class="title-slide slide level2">
<h2>CRF Theory</h2>

</section>
<section id="crf-motivation" class="slide level3">
<h3>CRF Motivation</h3>
<p><img data-src="static/hmm-memm-crf-diagrams.png"
style="width:65.0%" /></p>
<div>
<ul>
<li class="fragment"><em>Discriminative</em> as opposed to
<em>Generative</em></li>
<li class="fragment"><em>Bidirectional</em> influence from labels</li>
<li class="fragment"><em>Richer</em> and <em>Non-independent</em> word
features</li>
<li class="fragment">Solves the <em>“label bias”</em> issue of MEMM</li>
</ul>
</div>
<div class="footer">
<p>Illustration: <span class="citation"
data-cites="murphy2012machine">Murphy (2012)</span></p>
</div>
<aside class="notes">
<ul>
<li>HMM:
<ul>
<li>y_t is independent of all previous labels given y_t-1</li>
<li>x_t is independent of all previous labels and obs given y_t</li>
</ul></li>
<li>MEMM:
<ul>
<li>y_t is independent of all previous obs and labels given x_t and
y_t-1</li>
<li>x_t is independent of all other x’s</li>
</ul></li>
<li>Relaxed sequential Markov assumption
<ul>
<li>Bidirectional influence from labels</li>
</ul></li>
<li>Relax the “tag generates word” assumption
<ul>
<li>Allows rich word transformations</li>
</ul></li>
<li>Flexible influence from feature on</li>
<li>Relaxed Independence Assumptions
<ul>
<li>Non-independent features of the entire observation sequence</li>
</ul></li>
</ul>
<p>. Label Bias - MEMM are logistic regression for each state transition
given it’s current state - State transition with high probability
concentration - Leads to little influence from x-features - CRF solves
by normalizing over transitions over the whole sequence rather than each
step</p>
</aside>
</section>
<section id="discriminative-vs-generative-models" class="slide level3">
<h3>Discriminative VS Generative Models</h3>
<ul>
<li>Generative: <span class="math display">\[p(\mathbf{x},\mathbf{y}) =
p(\mathbf{y} \vert \mathbf{x})p(\mathbf{x})\]</span></li>
<li>Discriminative: <span class="math display">\[p(\mathbf{y} |
\mathbf{x})\]</span></li>
</ul>
<aside class="notes">

</aside>
</section>
<section id="generative-models-are-regularized" class="slide level3">
<h3>Generative Models are regularized</h3>
<ul>
<li>Generative: Maximize joint log-likelihood <span
class="math display">\[-\log p(\mathbf{x},\mathbf{y}) = -log
p(\mathbf{y} \vert \mathbf{x}) - \log p(\mathbf{x})\]</span></li>
<li>Discriminative: Maximize conditional log-likelihood: <span
class="math display">\[- \log p(\mathbf{y} | \mathbf{x})\]</span></li>
</ul>
</section>
<section id="discriminative-vs-generative-models-it-depends"
class="slide level3">
<h3>Discriminative VS Generative Models … it depends</h3>
<div class="columns">
<div class="column" style="width:50%;">
<figure>
<img data-src="static/gen-vs-disc-well-specified.png"
style="width:100.0%" alt="Well specified" />
<figcaption aria-hidden="true">Well specified</figcaption>
</figure>
</div><div class="column" style="width:50%;">
<figure>
<img data-src="static/gen-vs-disc-not-well-specified.png"
style="width:100.0%" alt="Not well specified" />
<figcaption aria-hidden="true">Not well specified</figcaption>
</figure>
</div>
</div>
<div class="footer">
<p>Source: <span class="citation" data-cites="ng2001discriminative">Ng
and Jordan (2001)</span>;<span class="citation"
data-cites="larochelleneuralnetworks">Larochelle (2013)</span></p>
</div>
</section>
<section id="discriminative" class="slide level3">
<h3>Discriminative</h3>
<ul>
<li>Markov Model: <span class="math display">\[p(Y_t) = \prod_{t=1}^T
p(y_t | y_{t-1})\]</span></li>
<li>Logistic regression: <span class="math display">\[p(y|x) =
\sigma(\theta^T x)\]</span></li>
</ul>
</section>
<section id="encoding-condition-dependence-as-graphs"
class="slide level3">
<h3>Encoding condition dependence as Graphs</h3>
<p><span class="math display">\[
p(a,b,c) = p(c | a, b)p(b | a)p(a)
\]</span></p>
<figure>
<img data-src="static/directed-graph.png"
alt="Conditional Decomposition" />
<figcaption aria-hidden="true">Conditional Decomposition</figcaption>
</figure>
</section>
<section class="slide level3">

<div class="callout callout-blue">
<h4>
Theorem
</h4>
<ul>
<li>Any decomposition of a joint probability function can be represented
as a DAG</li>
<li>Any DAG represents decomposition of a joint probability
function</li>
</ul>
</div>
</section>
<section id="crf-probabilistic-model" class="slide level3">
<h3>CRF probabilistic model</h3>
</section>
<section id="crf-log-linear-parametrization" class="slide level3">
<h3>CRF log-linear parametrization</h3>
</section>
<section id="fitting-crfs" class="slide level3">
<h3>Fitting CRF’s</h3>
<ul>
<li>How to include “high cardinality” features like current
<code>word</code></li>
</ul>
<p><span class="math display">\[ \ell(\theta) = \sum_{t=1}^{T} \log
p\left(\mathbf{y}_t \mid \mathbf{x}_t ; \theta\right)
+ c_1 \lVert \theta \rVert_1 + c_2 \lVert \theta \rVert_2 \]</span></p>
<ul>
<li>Use of Orthant-Wise Limited-memory Quasi-Newton (OWL-QN) <span
class="citation" data-cites="andrew2007scalable">(Andrew and Gao
2007)</span></li>
<li>Convex Optimization Problem</li>
</ul>
</section></section>
<section id="performance-comparison" class="title-slide slide level2">
<h2>Performance comparison</h2>
<div class="custom-small">
<table>
<colgroup>
<col style="width: 21%" />
<col style="width: 23%" />
<col style="width: 12%" />
<col style="width: 7%" />
<col style="width: 12%" />
<col style="width: 8%" />
<col style="width: 12%" />
</colgroup>
<thead>
<tr>
<th>Framework</th>
<th>Algorithm</th>
<th>CoNLL-2003</th>
<th>FIN</th>
<th>BioNLP2004</th>
<th>BC5CDR</th>
<th>MultiCoNER</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GliNER</td>
<td><strong>92.60</strong></td>
<td>-</td>
<td></td>
<td>88.70</td>
<td>-</td>
</tr>
<tr>
<td>Apache OpenNLP</td>
<td>Maximum Entropy</td>
<td>80.00</td>
<td>63.24</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Stanford CoreNLP</td>
<td>CRF</td>
<td>85.18</td>
<td>55.25</td>
<td><strong>73.26</strong></td>
<td>85.22</td>
<td>19.39</td>
</tr>
<tr>
<td>Flair</td>
<td>LSTM-CRF</td>
<td>90.35</td>
<td><strong>74.23</strong></td>
<td>71.64</td>
<td><strong>90.27</strong></td>
<td>56.27</td>
</tr>
<tr>
<td>spaCy</td>
<td>CNN-large</td>
<td>85.64</td>
<td>54.71</td>
<td>66.17</td>
<td>79.66</td>
<td>35.82</td>
</tr>
<tr>
<td>Hugging Face</td>
<td>roberta-base</td>
<td>89.92</td>
<td>63.18</td>
<td>66.56</td>
<td>87.08</td>
<td>55.21</td>
</tr>
<tr>
<td>Hugging Face</td>
<td>bert-base-cased</td>
<td>90.09</td>
<td>39.53</td>
<td>69.46</td>
<td>85.14</td>
<td><strong>56.64</strong></td>
</tr>
<tr>
<td>OpenAI</td>
<td>GPT-4</td>
<td>62.74</td>
<td>36.70</td>
<td>41.32</td>
<td>55.67</td>
<td>33.61</td>
</tr>
</tbody>
</table>
<p>Source: <span class="citation"
data-cites="keraghel2024survey">Keraghel, Morbieu, and Nadif
(2024)</span></p>
</div>
</section>

<section>
<section id="speed-comparison" class="title-slide slide level2">
<h2>Speed comparison</h2>

</section>
<section id="big-o-reminder" class="slide level3">
<h3>Big-O reminder</h3>
<div class="callout callout-blue">
<h4>
Definition
</h4>
<p><span class="math inline">\(f(n) = O(n)\)</span> <span
class="math inline">\(\\[10pt]\)</span> <span
class="math inline">\(\text{if} \quad f(n) \leq C\cdot n \qquad
\text{for all} \quad n&gt;n_0.\)</span></p>
</div>
</section>
<section id="inference-in-crf" class="slide level3">
<h3>Inference in CRF</h3>
<p><img data-src="static/trellis.png" style="width:40.0%" /></p>
<ul>
<li>Naive implementation: <span
class="math inline">\(O(2^n)\)</span></li>
</ul>
</section>
<section id="inference-in-transformers" class="slide level3">
<h3>Inference in Transformers</h3>
</section>
<section id="speed-benchmarks" class="slide level3">
<h3>Speed benchmarks</h3>
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Time per token</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">CRF</td>
<td style="text-align: right;">x</td>
</tr>
<tr>
<td style="text-align: left;">BERT</td>
<td style="text-align: right;">y</td>
</tr>
<tr>
<td style="text-align: left;">LSTM</td>
<td style="text-align: right;">z</td>
</tr>
</tbody>
</table>
</section>
<section id="conclusion" class="slide level3">
<h3>Conclusion</h3>
<ul>
<li>CRF’s are great at identifying entities which are identified by
syntactic and some extent semantic information</li>
<li>Quadratic transformers are MUCH MUCH slower, Sub-quadratic
transformers are also MUCH slower. The constant in front of n actually
matters!</li>
</ul>
</section></section>
<section id="references"
class="title-slide slide level2 allowframebreaks">
<h2 class="allowframebreaks">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" role="list">
<div id="ref-andrew2007scalable" class="csl-entry" role="listitem">
Andrew, Galen, and Jianfeng Gao. 2007. <span>“Scalable Training of l
1-Regularized Log-Linear Models.”</span> In <em>Proceedings of the 24th
International Conference on Machine Learning</em>, 33–40.
</div>
<div id="ref-bengio2000neural" class="csl-entry" role="listitem">
Bengio, Yoshua, Réjean Ducharme, and Pascal Vincent. 2000. <span>“A
Neural Probabilistic Language Model.”</span> <em>Advances in Neural
Information Processing Systems</em> 13.
</div>
<div id="ref-church1989stochastic" class="csl-entry" role="listitem">
Church, Kenneth Ward. 1989. <span>“A Stochastic Parts Program and Noun
Phrase Parser for Unrestricted Text.”</span> In <em>International
Conference on Acoustics, Speech, and Signal Processing,</em> 695–98.
IEEE.
</div>
<div id="ref-derose1988grammatical" class="csl-entry" role="listitem">
DeRose, Steven J. 1988. <span>“Grammatical Category Disambiguation by
Statistical Optimization.”</span> <em>Computational Linguistics</em> 14
(1): 31–39.
</div>
<div id="ref-huang2015bidirectional" class="csl-entry" role="listitem">
Huang, Zhiheng, Wei Xu, and Kai Yu. 2015. <span>“Bidirectional LSTM-CRF
Models for Sequence Tagging.”</span> <em>arXiv Preprint
arXiv:1508.01991</em>.
</div>
<div id="ref-keraghel2024survey" class="csl-entry" role="listitem">
Keraghel, Imed, Stanislas Morbieu, and Mohamed Nadif. 2024. <span>“A
Survey on Recent Advances in Named Entity Recognition.”</span> <em>arXiv
Preprint arXiv:2401.10825</em>.
</div>
<div id="ref-lafferty2001conditional" class="csl-entry" role="listitem">
Lafferty, John, Andrew McCallum, Fernando Pereira, et al. 2001.
<span>“Conditional Random Fields: Probabilistic Models for Segmenting
and Labeling Sequence Data.”</span> In <em>Icml</em>, 1:3. 2.
Williamstown, MA.
</div>
<div id="ref-larochelleneuralnetworks" class="csl-entry"
role="listitem">
Larochelle, Hugo. 2013. <span>“Neural Networks.”</span> <a
href="https://larocheh.github.io/neural_networks/content.html"
class="uri">https://larocheh.github.io/neural_networks/content.html</a>.
</div>
<div id="ref-mikolov2013efficient" class="csl-entry" role="listitem">
Mikolov, Tomas. 2013. <span>“Efficient Estimation of Word
Representations in Vector Space.”</span> <em>arXiv Preprint
arXiv:1301.3781</em>.
</div>
<div id="ref-murphy2012machine" class="csl-entry" role="listitem">
Murphy, Kevin P. 2012. <em>Machine Learning: A Probabilistic
Perspective</em>. MIT press.
</div>
<div id="ref-ng2001discriminative" class="csl-entry" role="listitem">
Ng, Andrew, and Michael Jordan. 2001. <span>“On Discriminative Vs.
Generative Classifiers: A Comparison of Logistic Regression and Naive
Bayes.”</span> <em>Advances in Neural Information Processing
Systems</em> 14.
</div>
<div id="ref-pennington2014glove" class="csl-entry" role="listitem">
Pennington, Jeffrey, Richard Socher, and Christopher D Manning. 2014.
<span>“Glove: Global Vectors for Word Representation.”</span> In
<em>Proceedings of the 2014 Conference on Empirical Methods in Natural
Language Processing (EMNLP)</em>, 1532–43.
</div>
<div id="ref-rabiner1989tutorial" class="csl-entry" role="listitem">
Rabiner, Lawrence R. 1989. <span>“A Tutorial on Hidden Markov Models and
Selected Applications in Speech Recognition.”</span> <em>Proceedings of
the IEEE</em> 77 (2): 257–86.
</div>
<div id="ref-ratnaparkhi1996maximum" class="csl-entry" role="listitem">
Ratnaparkhi, Adwait. 1996. <span>“A Maximum Entropy Model for
Part-of-Speech Tagging.”</span> In <em>Conference on Empirical Methods
in Natural Language Processing</em>.
</div>
<div id="ref-sang2003introduction" class="csl-entry" role="listitem">
Sang, Erik F, and Fien De Meulder. 2003. <span>“Introduction to the
CoNLL-2003 Shared Task: Language-Independent Named Entity
Recognition.”</span> <em>arXiv Preprint Cs/0306050</em>.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, A. 2017. <span>“Attention Is All You Need.”</span> <em>Advances
in Neural Information Processing Systems</em>.
</div>
</div>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^4//dist/reveal.js"></script>
  

  <!-- reveal.js plugins -->
  <script src="https://unpkg.com/reveal.js@^4//plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/zoom/zoom.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/math/math.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js-mermaid-plugin@2.3.0/plugin/mermaid/mermaid.js"></script>
  <!-- Chart plugin -->
  <script src="https://cdn.jsdelivr.net/npm/reveal.js-plugins@latest/chart/plugin.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/3.2.0/chart.min.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'linear',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },
        // mermaid initialize config
        mermaid: {
          // flowchart: {
          //   curve: 'linear',
          // },
        },
        chart: {
          defaults: {
            color: 'lightgray', // color of labels
            scale: {
              beginAtZero: true,
              ticks: { stepSize: 1 },
              grid: { color: "lightgray" }, // color of grid lines
            },
          },
          line: { borderColor: ["rgba(20,220,220,.8)", "rgba(220,120,120,.8)", "rgba(20,120,220,.8)"], "borderDash": [[0, 0], [0, 0]] },
          bar: { backgroundColor: ["rgba(20,220,220,.8)", "rgba(220,120,120,.8)", "rgba(20,120,220,.8)"]},
          pie: { backgroundColor: [["rgba(0,0,0,.8)", "rgba(220,20,20,.8)", "rgba(20,220,20,.8)", "rgba(220,220,20,.8)", "rgba(20,20,220,.8)"]] },
        },
        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom,
          RevealMermaid,
          RevealChart
        ]
      });
    </script>
    </body>
</html>
