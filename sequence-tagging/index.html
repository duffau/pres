<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="dcterms.date" content="2024-09-05">
  <title>A Blast from the Past</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reset.css">
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/reveal.css">
  <style>
    .reveal .sourceCode {  /* see #7635 */
      overflow: visible;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    /* The extra [class] is a hack that increases specificity enough to
       override a similar rule in reveal.js */
    ul.task-list[class]{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      font-size: inherit;
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
      margin-bottom: 0em;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="https://unpkg.com/reveal.js@^4//dist/theme/black.css" id="theme">
  <link rel="stylesheet" href="static/custom.css"/>
  <link rel="stylesheet" href="static/custom-elements.css"/>
<!-- tikzjax -->
<link rel="stylesheet" type="text/css" href="https://tikzjax.com/v1/fonts.css">
<script src="https://tikzjax.com/v1/tikzjax.js"></script>
<!-- highlight.js -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/vs2015.css">
<script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>
<script>hljs.highlightAll();</script>
  <meta property="og:url" content="https://duffau.github.io/talks/sequence-tagging/">
  <meta property="og:type" content="website">
  <meta property="og:image" content="https://duffau.github.io/talks/sequence-tagging/static/crf.svg" />
  <meta property="og:title" content="CRF's a Blast from the Past" />
  <meta property="og:description" content="Conditional Random Field models, which where big in the early 2000's, are light weight fast when it comes to sequence tagging. In this talk we investigate how they stack up against classical Transformers and LLMs, both in terms of accuracy and speed.">
  <meta name="twitter:card" content="summary_large_image">
  <meta property="twitter:domain" content="duffau.github.io">
  <meta property="twitter:url" content="https://duffau.github.io/talks/sequence-tagging/">
  <meta name="twitter:title" content="CRF's a Blast from the Past">
  <meta name="twitter:description" content="Conditional Random Field models, which where big in the early 2000's, are light weight fast when it comes to sequence tagging. In this talk we investigate how they stack up against classical Transformers and LLMs, both in terms of accuracy and speed.">
  <meta name="twitter:image" content="https://duffau.github.io/talks/sequence-tagging/static/crf.svg">
</head>
<body>
<div class="watermark">
<p><a href="https://alipes.dk/" rel="noopener noreferrer"> <img
      src="static/alipes-logo.svg"
    /> </a></p>
</div>
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="./static/front-cover.svg" data-background-opacity="0.2" data-background-size="80%">
  <h1 class="title">A <em>Blast</em> from the <em>Past</em></h1>
  <p class="subtitle"><em>Fast</em> Text Tagging with Conditional Random
Fields</p>
  <p class="date">2024-09-05</p>
</section>

<section id="about-me" class="slide level3">
<h3>About Me</h3>
<div>
<ul>
<li class="fragment">Senior Machine Learning Scientist at Alipes
<ul>
<li class="fragment">News Trading Algorithms</li>
<li class="fragment">Applied NLP and Machine Learning</li>
</ul></li>
<li class="fragment">Msc. in Economics from University of Copenhagen
from 2016
<ul>
<li class="fragment">Fell into the “<span
class="math inline">\(Math\)</span> and <code>Code</code>” pot at
University</li>
</ul></li>
<li class="fragment">Before joining Alipes
<ul>
<li class="fragment">Automated Sports Betting</li>
<li class="fragment">Co-founded a Machine Learning Consultancy</li>
<li class="fragment">Co-founded a NLP-powered Legal Tech start-up</li>
<li class="fragment">McKinsey as Data Scientist/ML Engineer</li>
</ul></li>
</ul>
</div>
</section>
<section id="slides" class="slide level3">
<h3>Slides</h3>
<p><img data-src="./static/talk-url-qr-code.svg" style="width:40.0%" />
<a
href="https://duffau.github.io/talks/sequence-tagging">duffau.github.io/talks/sequence-tagging</a></p>
</section>
<section id="sequence-tagging" class="slide level3">
<h3>Sequence Tagging</h3>
<pre class="txt"><code>The price of the [Pizza Margherita] is [10 dollars]. 
                  FOOD                  AMOUNT</code></pre>
</section>
<section id="section" class="slide level3">
<h3></h3>
<h4 id="part-of-speech-pos">Part-of-Speech (POS)</h4>
<pre class="txt"><code>Jim   worked at    Acme Corp. near the beautiful London Bridge.
NOUN  VERB   PREP  NOUN NOUN  PREP DET ADJ       NOUN   NOUN  EOS</code></pre>
<h4 id="named-entity-recognition-ner">Named Entity Recognition
(NER)</h4>
<pre class="txt"><code>Jim   worked at    Acme Corp. near the beautiful London Bridge.
PER   O      O     ORG  ORG   O    O   O         LOC    LOC   EOS</code></pre>
<p><span class="math display">\[\begin{aligned}
\text{Labels}:\quad \mathbf{y} &amp;= \{y_1, y_2, \ldots, y_T\}\\
\text{Features}:\quad \mathbf{x} &amp;= \{\mathbf{x}_1, \mathbf{x}_2,
\ldots, \mathbf{x}_T\}
\end{aligned}\]</span></p>
<aside class="notes">
<ul>
<li>Emphasis on sequential dependencies on labels</li>
<li>Mention the tag types
<ul>
<li>NER: “O”, “PER”, “LOC”, “ORG”</li>
</ul></li>
</ul>
</aside>
</section>
<section id="challenges-for-ner" class="slide level3">
<h3>Challenges for NER</h3>
<div>
<ul>
<li class="fragment"><strong>Challenge 1</strong>: Named entity
<strong>strings are rare</strong>
<ul>
<li class="fragment">Learn entity labels based on context and word
derived features</li>
</ul></li>
<li class="fragment"><strong>Challenge 2</strong>: Neighboring named
entity <strong>labels are dependent</strong>
<ul>
<li class="fragment">New York -&gt; <code>LOC</code></li>
<li class="fragment">New York Times -&gt; <code>ORG</code></li>
</ul></li>
</ul>
</div>
</section>
<section id="evolution-of-nlp-and-sequence-tagging"
class="title-slide slide level2">
<h2>Evolution of NLP and Sequence tagging</h2>
<div class="mermaid">
<pre>
%%{init: {'theme': 'forest', 'themeVariables': { 'fontSize': '24px', 'fontFamily': 'Source Sans Pro','cScale0': '#33B0F9', 'cScaleLabel0': '#ffffff',
              'cScale1': '#237cb0','cScaleLabel1': '#ffffff',
              'cScale2': '#125f8c', 'cScaleLabel2': '#ffffff'}}}%%
timeline
    section 1950-1990 Knowledge-Based Methods
        1950s : 1954 IBM-Georgetown machine translation - Sixty Russian sentences translated into English
        1960s : Slow progress in machine translation
              : 1966 ALPAC report leads to defunding of machine translation in the US
        1974-1980 : First AI Winter

    section 1990-2000 Statistical Feature-Engineered Methods
        Markov models
          : 1988 Markov models for PoS tagging
          : 1996 Maximum Entropy Markov Model (MEMM) published
          : 2001 Conditional Random Fields (CRF) introduced
        Data
          : 1985 WordNet - Princeton
          : 1993 Penn Treebank Project - 1 mio tokens from WSJ

    section 2010-today Deep Learning-Based Methods
        Emnbeddings and RNN's
          : 2013 Word Embeddings (Word2Vec, GloVe)
          : 2015 Neural Net Revolution (BiLSTM-CRF)
        Transformers
           : 2017 "Attention is all you need" Introduction of the Transformer model
           : 2020 Large-Scale Pre-trained Language Models (GPT-3)
        LLM
           : 2022 LLMs GPT 3.5 and ChatGPT
           : 2023 LLM zoo and Few-Shot Adaptation
</pre>
</div>
<aside class="notes">
<ul>
<li>1954 IBM-Georgtown: “within three or five years, machine translation
could well be a solved” problem.</li>
<li>HMM: The first to encode sequential information for PoS</li>
<li>Rabiner: HMM applied to speech since th 70’s but only widely know in
“recent years”
<ul>
<li>33,500 citation</li>
</ul></li>
<li>Penn Treebank Project: 1 mio annotated tokens from WSJ</li>
<li>MUC 6: https://aclanthology.org/volumes/M95-1/
<ul>
<li>Arranged by Naval Command and DARPA</li>
<li>A information extraction competition 10-20 participants from
industry adn universities 6 months work</li>
<li>Defined evalution methods and metrics like precision, recall and
f1</li>
<li>Task:
<ul>
<li>MUC-4: “Terrorist activities in Latin America”</li>
<li>MUC-6: “Negotiation of Labor Disputes”</li>
</ul></li>
<li>Coining the “NAmed Entity” at MUC-6</li>
</ul></li>
<li>MEMM:
<ul>
<li>Discriminative markov model - More direct model</li>
<li>Allows influence from features at any point in time</li>
</ul></li>
<li>Condtional Random Fields:
<ul>
<li>Sequential like HMM and MEMM</li>
<li>Are discriminative (like MEMM)</li>
<li>Allows influence from features at any point in time (Like MEMM)</li>
<li>Solves the “label bias” issue of MEMM’s</li>
</ul></li>
</ul>
</aside>
</section>

<section id="current-trend" class="title-slide slide level2">
<h2>Current Trend</h2>
<div style="height:400px">
<canvas data-chart="line">
<!--
{
 "data": {
  "labels": [2020,2021,2022,2023,2024],
  "datasets":[
   {
    "data":[86,88,43,41,18],
    "label":"POS - Penn Treebank",
    "yAxisID": "y",
    "fill": false
   },
   {
    "data":[63,112,105,156, 195],
    "label":"QA - TriviaQA (Wiki + Web)",
    "yAxisID": "y1"
   },
   {
    "data":[16,30,54,160,327],
    "label":"NLI - HellaSwag Sentence Completion",
    "yAxisID": "y1"
   }
  ]
 },
 "options": {
  "scales": {
   "y": {
    "type": "linear",
    "display": true,
    "position": "left",
    "title": {
     "display": true,
     "text": "Published Papers"
    }
   },
   "y1": {
    "type": "linear",
    "display": true,
    "position": "right",
    "title": {
     "display": true,
     "text": "Published Papers"
    },
    "grid": {
     "drawOnChartArea": false
    }
   }
  }
 }
}
-->
</canvas>
</div>
<p>Abstract tasks have taken over lower level tasks</p>
<div class="footer">
<p>Source: https://paperswithcode.com/datasets</p>
</div>
<aside class="notes">
<ul>
<li>Penn Tree Bank
<ul>
<li>Penn State Tree Bank</li>
<li>Initially released in 1992</li>
<li>First richly annotated text corpus</li>
<li>1 mio Annotated tokens (2500 stories) from Wall Street Journal
Article from 1989 Wall Street Journal</li>
<li>2022: Sequence Aligment Ensemble-BART encoder: 98.15 Accuracy
<ul>
<li>Ensemble of BART models</li>
<li>Weighted voting where weights a proportional to avg. alignment score
with other predictions in ensemble<br />
</li>
</ul></li>
<li>2018: BI-LSTM: 97.96 Accuracy</li>
</ul></li>
<li>TriviaQA: Challenging than QA pairs
<ul>
<li>Long context</li>
<li>Answers not optained by span prediction in question or context</li>
<li>2017 University of Washington NLP</li>
<li>Claude 5 shots: 87.5 f1 score</li>
<li>https://paperswithcode.com/sota/question-answering-on-triviaqa</li>
</ul></li>
<li>HellaSwag: Common sense Natural Language Inference
<ul>
<li>“A woman sits at a piano,” -&gt; “She sets her fingers on the
keys.”</li>
<li>Humans have 95% accuracy</li>
<li>From Allen Institute for AI a Non-Profit research org.</li>
<li>GPT4 10 shots: 95.3 Accuracy</li>
<li>https://paperswithcode.com/sota/sentence-completion-on-hellaswag</li>
</ul></li>
</ul>
</aside>
</section>

<section>
<section id="what-is-a-conditional-random-field"
class="title-slide slide level2">
<h2>What is a Conditional Random Field?</h2>
<div>
<ul>
<li class="fragment">Let’s have a look at a Hidden Markov Model 😅</li>
</ul>
</div>
</section>
<section id="hidden-markov-model" class="slide level3">
<h3>Hidden Markov Model</h3>
<p><img data-src="static/hmm-cont.svg" style="width:60.0%" /></p>
<ul>
<li>Sequential Model:
<ul>
<li>Observations: <span class="math inline">\(\mathbf{x} = \{x_1,
\ldots, x_T\}\)</span></li>
<li>Hidden states: <span class="math inline">\(\mathbf{y} = \{y_1,
\ldots, y_T\}\)</span></li>
</ul></li>
</ul>
</section>
<section id="hidden-markov-model-1" class="slide level3">
<h3>Hidden Markov Model</h3>
<p><img data-src="static/hmm-cont.svg" style="width:60.0%" /></p>
<ul>
<li><span class="math inline">\(y_t\)</span> are drawn from a set of
<span class="math inline">\(M\)</span> labels:
<ul>
<li>e.g. <code>[O, PER, LOC, ORG]</code></li>
</ul></li>
<li><span class="math inline">\(x_t\)</span> represents the word
identity at step <span class="math inline">\(t\)</span></li>
</ul>
</section>
<section id="hidden-markov-model-2" class="slide level3">
<h3>Hidden Markov Model</h3>
<p><img data-src="static/hmm-cont.svg" style="width:60.0%" /></p>
<ul>
<li><span class="math inline">\(p(y_t|y_{t-1}, y_{t-2}, \ldots, y_1) =
p(y_t|y_{t-1})\)</span>
<ul>
<li>Label depends only on <em>immediate predecessor</em></li>
</ul></li>
<li><span class="math inline">\(x_t\)</span> only depend on <span
class="math inline">\(y_t\)</span> <span class="math display">\[
p(\mathbf{y}, \mathbf{x}) = \prod_{t=1}^T p(y_t|y_{t-1})p(x_t|y_t).
\]</span></li>
</ul>
</section>
<section class="slide level3">

<ul>
<li>Transition probabilities <span
class="math inline">\(p(y_t|y_{t-1})\)</span> are <em>constant</em></li>
</ul>
<table>
<thead>
<tr>
<th></th>
<th>PER</th>
<th>LOC</th>
<th>ORG</th>
<th>O</th>
</tr>
</thead>
<tbody>
<tr>
<td>PER</td>
<td>0.8</td>
<td>0.01</td>
<td>0.01</td>
<td>0.18</td>
</tr>
<tr>
<td>LOC</td>
<td>0.02</td>
<td>0.65</td>
<td>0.05</td>
<td>0.28</td>
</tr>
<tr>
<td>ORG</td>
<td>0.01</td>
<td>0.01</td>
<td>0.3</td>
<td>0.68</td>
</tr>
<tr>
<td>O</td>
<td>0.05</td>
<td>0.1</td>
<td>0.1</td>
<td>0.75</td>
</tr>
</tbody>
</table>
</section>
<section id="crf-motivation" class="slide level3">
<h3>CRF Motivation</h3>
<div class="columns">
<div class="column" style="width:50%;">
<figure>
<img data-src="static/hmm.svg" style="width:100.0%" alt="HMM" />
<figcaption aria-hidden="true">HMM</figcaption>
</figure>
</div><div class="column" style="width:50%;">
<figure>
<img data-src="static/crf.svg" style="width:100.0%"
alt="Linear Chain CRF" />
<figcaption aria-hidden="true">Linear Chain CRF</figcaption>
</figure>
</div>
</div>
<div>
<ul>
<li class="fragment"><em>Discriminative</em> as opposed to
<em>Generative</em></li>
<li class="fragment"><em>Richer</em> and <em>overlapping</em> word
features</li>
</ul>
</div>
<aside class="notes">
<ul>
<li>Relax the “tag generates word” assumption
<ul>
<li>Allows rich word transformations</li>
<li>Rare words (e.g. proper names) will not have occurred in the
training set</li>
<li>Word identity feature is uninformative</li>
</ul></li>
<li>Relaxed sequential Markov assumption
<ul>
<li>Bidirectional influence from labels</li>
<li>Time varying transition probabilities</li>
</ul></li>
<li>Relaxed Independence Assumptions
<ul>
<li>Non-independent features of the entire observation sequence</li>
</ul></li>
</ul>
<p>. Label Bias - MEMM are logistic regression for each state transition
given it’s current state - State transition with high probability
concentration - Leads to little influence from x-features - CRF solves
by normalizing over transitions over the whole sequence rather than each
step</p>
</aside>
</section>
<section id="linear-chain-crf-definition" class="slide level3">
<h3>Linear Chain CRF definition</h3>
<p><span class="math display">\[
p(\mathbf{y}|\mathbf{x}) = \frac{1}{Z(\mathbf{x})} \prod_{t=1}^T \exp
\left( \sum_k \theta_k f_k(y_t, y_{t-1}, \mathbf{x}_t) \right)
\]</span></p>
<p>where,</p>
<ul>
<li><span class="math inline">\(\theta_k \in \mathbb{R}\)</span></li>
<li><span class="math inline">\(f_k\)</span> is a real-valued feature
function</li>
<li><span class="math inline">\(k\)</span> ranges over transitions <span
class="math inline">\((i,j)\)</span> and state-observation pairs <span
class="math inline">\((i,o)\)</span>.</li>
<li><span class="math inline">\(\mathbf{x}_t\)</span> is a feature
vector at time <span class="math inline">\(t\)</span></li>
</ul>
</section>
<section id="discriminative-vs-generative" class="slide level3">
<h3>Discriminative VS Generative</h3>
<div>
<ul>
<li class="fragment">Generative: <span class="math display">\[p(y,
\mathbf{x}) = p(\mathbf{y} \vert \mathbf{x})p(\mathbf{x})\]</span>
<ul>
<li class="fragment"><span class="math inline">\(p(\mathbf{x})\)</span>
is often difficult to model -&gt; Simplifying assumptions</li>
</ul></li>
<li class="fragment">Discriminative: <span class="math display">\[p(y |
\mathbf{x})\]</span>
<ul>
<li class="fragment">No need to model <span
class="math inline">\(p(\mathbf{x})\)</span></li>
</ul></li>
</ul>
</div>
<aside class="notes">
<ul>
<li>Generative:: Label vector y can probabilistically “generate” a
feature vector x
<ul>
<li>By modelling p(y, x) = p(y) p(x | y)</li>
<li>p(x | y) generates the features given the (hidden) labels</li>
</ul></li>
<li>Discriminative: How a feature vector x and assign it a label y
<ul>
<li>models the “descision rule” directly</li>
</ul></li>
<li>Generative models “too much” by specifying p(x)</li>
<li>Discriminative models perform worse if the generative model is the
true model</li>
</ul>
</aside>
</section>
<section id="discriminative-vs-generative-1" class="slide level3">
<h3>Discriminative VS Generative</h3>
<ul>
<li>Generative - Naive Bayes:
<ul>
<li><span class="math inline">\(p(y, \mathbf{x}) = p(y;\theta)
\prod_{k=1}^K p(x_k | y;\theta)\)</span></li>
<li>Assumes <span class="math inline">\(p(x_k| x_{k+1}, \ldots, x_K, y)
= p(x_k| y)\)</span></li>
</ul></li>
<li>Discriminative - Logistic regression:
<ul>
<li><span class="math inline">\(p(y | \mathbf{x};\theta) = 1/(1 +
e^{\theta^\top \mathbf{x}})\)</span></li>
<li>No assumption on <span
class="math inline">\(\mathbf{x}\)</span></li>
</ul></li>
</ul>
</section>
<section id="paths-to-linear-chain-crf" class="slide level3">
<h3>Paths to Linear Chain CRF</h3>
<p><img data-src="static/model-relation.png" style="width:40.0%" /></p>
<div class="footer">
<p>Illustration: <span class="citation"
data-cites="sutton2012introduction">Sutton, McCallum, et al.
(2012)</span></p>
</div>
</section>
<section id="crf-from-hmms-cond.-distribution" class="slide level3">
<h3>CRF from HMM’s cond. distribution</h3>
<div class="custom-medium">
<span class="math display">\[\begin{aligned}
p(\mathbf{y}, \mathbf{x}) &amp;= \prod_{t=1}^T p(y_t|y_{t-1})p(x_t|y_t)
\\
&amp;= \frac{1}{Z} \prod_{t=1}^{T} \exp (\sum_{i,j \in S} \theta_{ij}
\mathbf{1}_{\{y_t = i\}} \mathbf{1}_{\{y_{t-1} = j\}} \\
&amp;+ \sum_{i \in S} \sum_{o \in O} \mu_{oi} \mathbf{1}_{\{y_t = i\}}
\mathbf{1}_{\{x_t = o\}})\\
\end{aligned}\]</span>
<span class="math display">\[\begin{aligned}
\theta_{ij} &amp;= \log p(y^\prime=i|y=j)\quad \text{transition prob.}
\\
\mu_{oi} &amp;= \log p(x=o | y=i)\quad \text{word give label prob.} \\
Z &amp;= 1
\end{aligned}\]</span>
</div>
<aside class="notes">
<ul>
<li>Every homogeneous HMM can be written on this form</li>
<li>Every distribution which factorizes as above is a HMM</li>
</ul>
</aside>
</section>
<section id="crf-from-hmms-cond.-distribution-1" class="slide level3">
<h3>CRF from HMM’s cond. distribution</h3>
<div class="custom-medium">
We introduce feature functions:
<span class="math display">\[\begin{aligned}
f_{ij}(y, y^\prime, x) &amp;= f_{ij}(y, y^\prime) = \mathbf{1}_{\{y_t =
i\}} \mathbf{1}_{\{y_{t-1} = j\}} \\
f_{io}(y, y^\prime, x) &amp;= f_{io}(y, x) = \mathbf{1}_{\{y_t = i\}}
\mathbf{1}_{\{x_t = o\}}
\end{aligned}\]</span>
<p><span class="math inline">\(f_k\)</span> indexes over all <span
class="math inline">\(f_{io}\)</span> and <span
class="math inline">\(f_{ij}\)</span></p>
<span class="math display">\[\begin{aligned}
p(\mathbf{y} , \mathbf{x}) = \frac{1}{Z} \prod_{t=1}^{T} \exp \left(
\sum_{k=1}^K \theta_k f_k(y_t, y_{t-1}, x_t) \right)
\end{aligned}\]</span>
</div>
</section>
<section id="crf-from-hmms-cond.-distribution-2" class="slide level3">
<h3>CRF from HMM’s cond. distribution</h3>
<div class="custom-medium">
<span class="math display">\[\begin{aligned}
p(\mathbf{y} | \mathbf{x}) = \frac{\prod_{t=1}^{T} \exp \left(
\sum_{k=1}^K \theta_k f_k(y_t, y_{t-1}, x_t)
\right)}{\sum_{\mathbf{y}^\prime} \prod_{t=1}^{T} \exp \left(
\sum_{k=1}^K \theta_k f_k(y_t, y_{t-1}, x_t) \right)}
\end{aligned}\]</span>
<div>
<ul>
<li class="fragment">The above is a CRF factorization 🎉</li>
<li class="fragment">But is restricted in two major ways:
<ul>
<li class="fragment">Indicator only feature functions</li>
<li class="fragment">Only the current word identity <span
class="math inline">\(x_t\)</span> enters the model</li>
</ul></li>
</ul>
</div>
</div>
<div class="fragment">
<p><strong>Take away</strong>: HMM’s are CRF’s but with (much) more
restricted feature functions</p>
</div>
</section>
<section id="crf-from-logistic-regression" class="slide level3">
<h3>CRF from logistic regression</h3>
<span class="math display">\[\begin{aligned}
p(\mathbf{y} | \mathbf{x})_{logistic} &amp;= \prod_{t=1}^{T} p(y_t |
\mathbf{x}_t)
= \prod_{t=1}^{T} \frac{\exp (\theta_y + \sum_k \theta_{y,k} x_k) }{
\sum_{y^\prime} \exp (\theta_{y^\prime} + \sum_k \theta_{y^\prime,k}
x_k)} \\
&amp;= \frac{1}{Z(\mathbf{x})} \prod_{t=1}^{T} \exp (\theta_y + \sum_k
\theta_{y,k} x_k) \\
&amp;= \frac{1}{Z(\mathbf{x})} \prod_{t=1}^{T} \exp (\sum_{k}
\theta_{y,k} f_{k}(x_k, y_t))
\end{aligned}\]</span>
</section>
<section id="crf-from-logistic-regression-1" class="slide level3">
<h3>CRF from logistic regression</h3>
Adding transition matrix
<span class="math display">\[\begin{aligned}
p(\mathbf{y} | \mathbf{x})_{CRF} &amp;= \frac{1}{Z(\mathbf{x})}
\prod_{t=1}^{T} \exp (\sum_{k} \theta_{y,k} f_{k}(x_k, y_t) +
V_{y_{t-1},y_{t}})
\end{aligned}\]</span>
</section></section>
<section>
<section id="crf-training-demo" class="title-slide slide level2">
<h2>CRF Training Demo</h2>
<p><a
href="https://github.com/duffau/talks/tree/master/sequence-tagging/demo">github.com/duffau/talks/tree/master/sequence-tagging/demo</a></p>
</section>
<section id="crf-suite" class="slide level3">
<h3><a href="https://www.chokkan.org/software/crfsuite/">CRF
Suite</a></h3>
<div class="columns">
<div class="column">
<p><img data-src="static/Naoaki.png" /></p>
</div><div class="column">
<p><img data-src="static/crfsuite.png" /></p>
</div>
</div>
<ul>
<li>Published in 2007</li>
<li>C++ implementation of CRF training</li>
<li>Stable, robust and fast!</li>
</ul>
</section>
<section id="section-1" class="slide level3">
<h3></h3>
<pre class="py" data-include="./demo/fit.py" data-startFrom="1"
data-endAt="4"><code>import re
import datasets
import sklearn_crfsuite
from sklearn_crfsuite import metrics
</code></pre>
</section>
<section class="slide level3">

<pre class="py" data-include="./demo/fit.py" data-startFrom="7"
data-endAt="15"><code>def tokens_to_features(tokens, i):
    features = {
        &quot;bias&quot;: 1.0,
        &quot;word&quot;: tokens[i].lower(),
        &quot;prev_word&quot;: tokens[i - 1] if i&gt;0 else &quot;BOS&quot;,
        &quot;next_word&quot;: tokens[i + 1] if i&gt;len(tokens) else &quot;EOS&quot;,
        &quot;shape&quot;: re.sub(r&quot;\d&quot;, &quot;X&quot;, tokens[i]),
    }
    return features
</code></pre>
<pre class="python"><code># tokens
[&quot;Anders&quot;, &quot;loves&quot;, &quot;pizza&quot;, &quot;from&quot;, &quot;Rome&quot;]</code></pre>
<pre class="python"><code># features i=2
{&quot;bias&quot;:1.0, &quot;word&quot;: &quot;pizza&quot;, &quot;prev_word&quot;: &quot;loves&quot;, &quot;next_word&quot;: &quot;from&quot;, &quot;shape&quot;: &quot;pizza&quot;}</code></pre>
</section>
<section class="slide level3">

<pre class="py" data-include="./demo/fit.py" data-startFrom="18"
data-endAt="28"><code>def load_X_y(dataset_id=&quot;eriktks/conll2003&quot;, split=&quot;train&quot;):
    data = datasets.load_dataset(dataset_id)
    sentences = data[split][&quot;tokens&quot;]
    labels = data[split][&quot;ner_tags&quot;]
    label_names = data[split].features[&quot;ner_tags&quot;].feature.names
    
    X, y = [], []
    for sentence, label_seq in zip(sentences, labels):
        X.append([tokens_to_features(sentence, i) for i in range(len(sentence))])
        y.append([label_names[label_id] for label_id in label_seq])
    return X, y
</code></pre>
</section>
<section class="slide level3">

<pre class="python"><code>X = [
      [
        {&quot;bias&quot;: 1.0, &quot;word&quot;: &quot;Anders&quot;, ...},
        ..., 
        {&quot;bias&quot;: 1.0, &quot;word&quot;: &quot;Rome&quot;, ...}
      ],
      ...
]

y = [
  [&quot;PER&quot;, ..., &quot;LOC&quot;],
  ...
]
</code></pre>
</section>
<section class="slide level3">

<pre class="py" data-include="./demo/fit.py" data-startFrom="31"
data-endAt="41"><code>X, y = load_X_y(split=&quot;train&quot;)
crf = sklearn_crfsuite.CRF(
    algorithm=&quot;lbfgs&quot;,
    c1=0.5,
    c2=0.01,
    all_possible_states=True,
    all_possible_transitions=True,
    max_iterations=100,
    verbose=True
)
crf.fit(X, y)
</code></pre>
<pre class="bash"><code>Iter 1   time=0.20 loss=221774.8 active=177764 feature_norm=1.00
Iter 2   time=0.11 loss=207529.7 active=153314 feature_norm=2.91
Iter 3   time=0.12 loss=172616.6 active=118451 feature_norm=2.39
...
Iter 100 time=0.12 loss=20369.2 active=24307 feature_norm=224.15
Total seconds required for training: 12.497</code></pre>
</section>
<section class="slide level3">

<h4 id="number-of-features">Number of features</h4>
<pre class="bash"><code>Number of active features: 24307 (574200)
Number of active attributes: 17166 (63791)
Number of active labels: 9 (9)

train n sentence: 14041
train n tokens:  203621</code></pre>
<pre class="python"><code># Worst case (all_possible_states=True and all_possible_transitions=True)
features = (number of attributes * number of labels) 
           + (number of labels * number of labels)</code></pre>
</section>
<section class="slide level3">

<pre class="py" data-include="./demo/fit.py" data-startFrom="49"
data-endAt="51"><code>X_test, y_test = load_X_y(split=&quot;test&quot;)
y_pred = crf.predict(X_test)
print(metrics.flat_classification_report(y_test, y_pred, labels=label_names))
</code></pre>
<pre class="bash"><code>              precision    recall  f1-score   support

       B-ORG       0.89      0.57      0.69      1661
      I-MISC       0.74      0.62      0.67       216
       B-LOC       0.89      0.80      0.85      1668
       B-PER       0.90      0.56      0.69      1617
       I-ORG       0.80      0.65      0.72       835
       I-LOC       0.79      0.69      0.74       257
       I-PER       0.89      0.68      0.77      1156
      B-MISC       0.86      0.68      0.76       702

   micro avg       0.87      0.66      0.75      8112
   macro avg       0.84      0.66      0.74      8112
weighted avg       0.87      0.66      0.74      8112</code></pre>
</section>
<section class="slide level3">

<h4 id="transition-weights">Transition weights</h4>
<pre class="bash"><code>Top likely transitions:
B-PER  -&gt; I-PER   6.683884
B-ORG  -&gt; I-ORG   5.801057
I-ORG  -&gt; I-ORG   5.443938
B-MISC -&gt; I-MISC  4.845435
I-MISC -&gt; I-MISC  4.664166

Top unlikely transitions:
B-LOC  -&gt; I-MISC  -5.871822
O      -&gt; I-PER   -5.968892
O      -&gt; I-LOC   -6.719913
O      -&gt; I-MISC  -7.026504
O      -&gt; I-ORG   -7.582135</code></pre>
</section>
<section class="slide level3">

<h4 id="state-feature-weights">State feature weights</h4>
<pre class="bash"><code>Top positive state features:
9.733456 I-MISC   shape:Index
8.026899 I-ORG    shape:Newsroom
7.925168 O        shape:said
7.584594 B-MISC   word:english
7.378129 I-MISC   word:classic

Top negative state features:
-4.852697 B-ORG    word:of
-5.058472 I-PER    prev_word:BOS
-5.093323 I-LOC    prev_word:BOS
-6.203149 I-ORG    prev_word:BOS
-6.461425 I-MISC   prev_word:BOS</code></pre>
</section>
<section class="slide level3">

<section style="font-size: 10px;">
</section>
<section id="fitting-crfs" class="slide level3">
<h3>Fitting CRF’s</h3>
<ul>
<li>Fit with “high cardinality” features like current <code>word</code>
<span class="math display">\[\ell(\theta) = \sum_{t=1}^{T} \log
p\left(\mathbf{y}_t \mid \mathbf{x}_t ; \theta\right) + c_1 \lVert
\theta \rVert_1 + c_2 \lVert \theta \rVert_2\]</span></li>
</ul>
<div class="columns">
<div class="column" style="width:20%;">
<p><img data-src="static/l1_l2_reg.svg" style="width:100.0%" /></p>
</div><div class="column" style="width:80%;">
<div>
<ul>
<li class="fragment">L2: Strictly Convex Optimization</li>
<li class="fragment">Efficient use of Quasi-Newton Methods</li>
<li class="fragment">L1: Shrinks the parameter space</li>
<li class="fragment">Hyperparameter opt. <span
class="math inline">\((c_1,c_2)\)</span></li>
</ul>
</div>
</div>
</div>
<aside class="notes">
<ul>
<li>With L1 use of Orthant-Wise Limited-memory Quasi-Newton</li>
<li>Only the linear chain lead to a strictly convex problem</li>
</ul>
</aside>
</section>
</section></section>
<section>
<section id="performance-comparison" class="title-slide slide level2">
<h2>Performance comparison</h2>

</section>
<section id="overall-performance" class="slide level3">
<h3>Overall Performance</h3>
<figure>
<img data-src="static/cd_diagram_ner_survey.png" style="width:60.0%"
alt="Mean rank of each model on 10 datasets" />
<figcaption aria-hidden="true">Mean rank of each model on 10
datasets</figcaption>
</figure>
<ul>
<li>Different domains and difficulties</li>
<li>CRF is from Stanford’s Java NLP library using <a
href="https://javadoc.io/static/edu.stanford.nlp/stanford-corenlp/1.2.0/edu/stanford/nlp/ie/NERFeatureFactory.html">comprehensive
list of engineered features</a></li>
<li>CRF does well. In the middle of the pack on average.</li>
</ul>
<div class="footer">
<p>Source: <span class="citation"
data-cites="keraghel2024survey">Keraghel, Morbieu, and Nadif
(2024)</span></p>
</div>
</section>
<section id="dataset-level-performance" class="slide level3">
<h3>Dataset level performance</h3>
<div class="custom-small">
<table>
<colgroup>
<col style="width: 18%" />
<col style="width: 19%" />
<col style="width: 10%" />
<col style="width: 9%" />
<col style="width: 9%" />
<col style="width: 10%" />
<col style="width: 10%" />
<col style="width: 10%" />
</colgroup>
<thead>
<tr>
<th>Framework</th>
<th>Algorithm</th>
<th>CoNLL-2003</th>
<th>WNUT2017</th>
<th>FIN</th>
<th>BioNLP2004</th>
<th>BC5CDR</th>
<th>MultiCoNER</th>
</tr>
</thead>
<tbody>
<tr>
<td></td>
<td>GliNER</td>
<td><strong>92.60</strong></td>
<td>-</td>
<td>-</td>
<td></td>
<td>88.70</td>
<td>-</td>
</tr>
<tr>
<td>Apache OpenNLP</td>
<td>Maximum Entropy</td>
<td>80.00</td>
<td>-</td>
<td>63.24</td>
<td>-</td>
<td>-</td>
<td>-</td>
</tr>
<tr>
<td>Stanford CoreNLP</td>
<td>CRF</td>
<td>85.18</td>
<td>8.34</td>
<td>55.25</td>
<td><strong>73.26</strong></td>
<td>85.22</td>
<td>19.39</td>
</tr>
<tr>
<td>Flair</td>
<td>LSTM-CRF</td>
<td>90.35</td>
<td>38.07</td>
<td><strong>74.23</strong></td>
<td>71.64</td>
<td><strong>90.27</strong></td>
<td>56.27</td>
</tr>
<tr>
<td>spaCy</td>
<td>CNN-large</td>
<td>85.64</td>
<td>9.78</td>
<td>54.71</td>
<td>66.17</td>
<td>79.66</td>
<td>35.82</td>
</tr>
<tr>
<td>Hugging Face</td>
<td>roberta-base</td>
<td>89.92</td>
<td>41.84</td>
<td>63.18</td>
<td>66.56</td>
<td>87.08</td>
<td>55.21</td>
</tr>
<tr>
<td>Hugging Face</td>
<td>bert-base-cased</td>
<td>90.09</td>
<td>33.32</td>
<td>39.53</td>
<td>69.46</td>
<td>85.14</td>
<td><strong>56.64</strong></td>
</tr>
<tr>
<td>OpenAI</td>
<td>GPT-4</td>
<td>62.74</td>
<td>18.82</td>
<td>36.70</td>
<td>41.32</td>
<td>55.67</td>
<td>33.61</td>
</tr>
</tbody>
</table>
</div>
<div class="footer">
<p>Source: <span class="citation"
data-cites="keraghel2024survey">Keraghel, Morbieu, and Nadif
(2024)</span></p>
</div>
</section>
<section id="performance-analysis" class="slide level3">
<h3>Performance analysis</h3>
<ul>
<li>CRF has best relative performance on <em>BioNLP2004</em>
<ul>
<li>Entity Types: DNA, protein, cell_type, cell_line, RNA</li>
<li><blockquote>
<p>A low [NM23.H1]DNA gene expression identifying high malignancy in
human melanomas.</p>
</blockquote></li>
</ul></li>
</ul>
</section>
<section id="performance-analysis-1" class="slide level3">
<h3>Performance analysis</h3>
<ul>
<li>CRF has worst relative performance on <em>WNUT 2017</em>
<ul>
<li>Entity types: Person, Location, Corporation, Consumer good, Creative
work, Group</li>
<li>“ability to detect and classify novel, emerging, singleton named
entities in noisy text.”</li>
<li><blockquote>
<p>Tweet: “so.. kktny[Creative work] in 30 mins?”</p>
</blockquote></li>
<li>*Kourtney And Kim Take New York</li>
</ul></li>
</ul>
</section>
<section id="abstraction-hierarchy" class="slide level3">
<h3>Abstraction Hierarchy</h3>
<p><img data-src="static/info_pyramid.svg" /></p>
<ul>
<li>CRF does not have semantic information e.g. embeddings</li>
<li>CRF features cannot generalize to unseen words or word contexts</li>
<li>CRFs are just linear machines, no chance of extrapolating abstract
information</li>
</ul>
</section></section>
<section>
<section id="speed-comparison" class="title-slide slide level2">
<h2>Speed comparison</h2>

</section>
<section id="big-o-reminder" class="slide level3">
<h3>Big-O reminder</h3>
<div class="callout callout-blue">
<h4>
Definition
</h4>
<p><span class="math inline">\(f(n) = O(n)\)</span> <span
class="math inline">\(\\[10pt]\)</span> <span
class="math inline">\(\text{if} \quad f(n) \leq C\cdot n \qquad
\text{for all} \quad n&gt;n_0.\)</span></p>
</div>
</section>
<section id="inference-in-crf" class="slide level3">
<h3>Inference in CRF</h3>
<ul>
<li>Partition function is a sum over all possible labeling <span
class="math display">\[ Z(X) = \sum_{y_1}\sum_{y_2}\ldots\sum_{y_T}
\prod_{t=1}^T \exp \left( \sum_k \theta_k f_k(y_t, y_{t-1},
\mathbf{x}_t) \right)\]</span></li>
</ul>
<p><img data-src="static/trellis.svg" style="width:50.0%" /></p>
<ul>
<li>Naive implementation: <span
class="math inline">\(O(M^T)\)</span></li>
</ul>
</section>
<section id="inference-in-transformers" class="slide level3">
<h3>Inference in Transformers</h3>
<table>
<thead>
<tr>
<th>Layer Type</th>
<th>Complexity per Layer</th>
</tr>
</thead>
<tbody>
<tr>
<td>Self-attention</td>
<td><span class="math inline">\(O(T^2*K)\)</span></td>
</tr>
<tr>
<td>Recurrent</td>
<td><span class="math inline">\(O(T*K^2)\)</span></td>
</tr>
<tr>
<td>CRF</td>
<td><span class="math inline">\(O(T*M^2)\)</span></td>
</tr>
</tbody>
</table>
<div class="footer">
<p>Source: <span class="citation"
data-cites="vaswani2017attention">Vaswani (2017)</span></p>
</div>
</section>
<section id="speed-benchmarks---ner-on-conll-2003" class="slide level3">
<h3>Speed benchmarks - NER on CoNLL 2003</h3>
<ul>
<li>Computed on CPU</li>
<li>Including tokenization and feature generation</li>
<li>Using “predict single” approach</li>
</ul>
</section>
<section id="speed-benchmarks---ner-on-conll-2003-1"
class="slide level3">
<h3>Speed benchmarks - NER on CoNLL 2003</h3>
<div class="custom-medium">
<table>
<thead>
<tr>
<th style="text-align: left;">Model</th>
<th style="text-align: right;">Params</th>
<th style="text-align: right;">Time per Sentence</th>
<th style="text-align: right;">Time per Token</th>
<th style="text-align: right;"></th>
<th style="text-align: right;"></th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align: left;">BERT<sup>1</sup></td>
<td style="text-align: right;">108.31 M</td>
<td style="text-align: right;">59.31 ms</td>
<td style="text-align: right;">4.09 ms</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
<tr>
<td style="text-align: left;">CRF<sup>2</sup></td>
<td style="text-align: right;">0.038 M</td>
<td style="text-align: right;">0.119 ms</td>
<td style="text-align: right;">0.00823 ms</td>
<td style="text-align: right;"></td>
<td style="text-align: right;"></td>
</tr>
</tbody>
</table>
</div>
<div class="footer">
<p><sup>1</sup>: kamalkraj/bert-base-cased-ner-conll2003 <sup>2</sup>:
Fitted CRF</p>
</div>
</section>
<section id="conclusion" class="slide level3">
<h3>Conclusion</h3>
<div>
<ul>
<li class="fragment">CRFs are great at identifying entities which are
<ul>
<li class="fragment">identified by <strong>syntactic</strong> and some
extent semantic information</li>
</ul></li>
<li class="fragment">CRFs are <strong>fast to train</strong>, which
enable a quick tag-train loop
<ul>
<li class="fragment">with few features they do well on small
datasets</li>
</ul></li>
<li class="fragment">CRFs inference is <strong>fast</strong>, especially
with C++ implementation</li>
</ul>
</div>
</section>
<section id="learn-more" class="slide level3">
<h3>Learn more</h3>
<div class="columns">
<div class="column" style="width:30%;">
<p><a
href="https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">Sutton
&amp; McCallum - Intro to CRF</a></p>
<p><img
data-src="static/front-page-sutton-mccallum-intro-crf.png" /></p>
</div><div class="column" style="width:30%;">
<p><a
href="https://www.youtube.com/playlist?list=PL6Xpj9I5qXYEcOhn7TqghAJ6NAPrNmUBH">Hugo
Larochelle - Neural Network Course</a></p>
<p><img data-src="static/front-page-hugo-nn-course.png" /></p>
</div><div class="column" style="width:30%;">
<p><a
href="https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf">Chap.8
Graphical Models</a></p>
<p><img data-src="static/front-page-bishop.jpg" /></p>
</div>
</div>
<section style="text-align: left;">
</section></section>
<section id="references"
class="title-slide slide level2 allowframebreaks">
<h2 class="allowframebreaks">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent"
data-entry-spacing="0" role="list">
<div id="ref-keraghel2024survey" class="csl-entry" role="listitem">
Keraghel, Imed, Stanislas Morbieu, and Mohamed Nadif. 2024. <span>“A
Survey on Recent Advances in Named Entity Recognition.”</span> <em>arXiv
Preprint arXiv:2401.10825</em>.
</div>
<div id="ref-sutton2012introduction" class="csl-entry" role="listitem">
Sutton, Charles, Andrew McCallum, et al. 2012. <span>“An Introduction to
Conditional Random Fields.”</span> <em>Foundations and
Trends<span></span> in Machine Learning</em> 4 (4): 267–373.
</div>
<div id="ref-vaswani2017attention" class="csl-entry" role="listitem">
Vaswani, A. 2017. <span>“Attention Is All You Need.”</span> <em>Advances
in Neural Information Processing Systems</em>.
</div>
</div>
</section>
</section>
    </div>
  </div>

  <script src="https://unpkg.com/reveal.js@^4//dist/reveal.js"></script>
  

  <!-- reveal.js plugins -->
  <script src="https://unpkg.com/reveal.js@^4//plugin/notes/notes.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/search/search.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/zoom/zoom.js"></script>
  <script src="https://unpkg.com/reveal.js@^4//plugin/math/math.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/reveal.js-mermaid-plugin@2.3.0/plugin/mermaid/mermaid.js"></script>
  <!-- Chart plugin -->
  <script src="https://cdn.jsdelivr.net/npm/reveal.js-plugins@latest/chart/plugin.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/Chart.js/3.2.0/chart.min.js"></script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'bottom-right',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'default',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: true,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'linear',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'fade',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },
        // mermaid initialize config
        mermaid: {
          // flowchart: {
          //   curve: 'linear',
          // },
        },
        chart: {
          defaults: {
            color: 'lightgray', // color of labels
            scale: {
              beginAtZero: true,
              ticks: { stepSize: 1 },
              grid: { color: "lightgray" }, // color of grid lines
            },
          },
          line: { borderColor: ["rgba(20,220,220,.8)", "rgba(220,120,120,.8)", "rgba(20,120,220,.8)"], "borderDash": [[0, 0], [0, 0]] },
          bar: { backgroundColor: ["rgba(20,220,220,.8)", "rgba(220,120,120,.8)", "rgba(20,120,220,.8)"]},
          pie: { backgroundColor: [["rgba(0,0,0,.8)", "rgba(220,20,20,.8)", "rgba(20,220,20,.8)", "rgba(220,220,20,.8)", "rgba(20,20,220,.8)"]] },
        },
        // reveal.js plugins
        plugins: [
          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom,
          RevealMermaid,
          RevealChart
        ]
      });
    </script>
    </body>
</html>
